{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import sys\n",
    "sys.path.append(\"..\") # Adds higher directory to python modules path for importing from src dir\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autotime\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 20.7 ms\n"
     ]
    }
   ],
   "source": [
    "HOME = \"..\"\n",
    "DATA_DIR = \"data\"\n",
    "REUTERS = os.path.join(HOME, DATA_DIR, \"preprocessed\", \"news_reuters.csv\")\n",
    "BLOOMBERG = os.path.join(HOME, DATA_DIR, \"preprocessed\", \"news_bloomberg.csv\")\n",
    "NEWS = os.path.join(HOME, DATA_DIR, \"preprocessed\", \"news.csv\")\n",
    "# Columns: 'date', 'filename', 'content'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 564 ms\n"
     ]
    }
   ],
   "source": [
    "from src.datasets import NyseSecuritiesDataset\n",
    "from src.datasets import NyseStocksDataset\n",
    "import src.nlp_utils as nlp_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 40.9 ms\n"
     ]
    }
   ],
   "source": [
    "securities_ds = NyseSecuritiesDataset(file_path='../data/nyse/securities.csv')\n",
    "companies = securities_ds.get_all_company_names()  # List[Tuple[symbol, name]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reuters:\n",
    "- 106519 articles\n",
    "- From 2006-10-20 to 2013-11-20\n",
    "- 45363 before 2010-01-04\n",
    "\n",
    "Bloomberg:\n",
    "- 448395 articles\n",
    "- From 2006-10-20 to 2013-11-26\n",
    "- 1148 before 2010-01-04\n",
    "\n",
    "Nyse:\n",
    "- From 2010-01-04 to 2016-12-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_r = pd.read_csv(REUTERS, usecols=[0, 1], index_col=0)\n",
    "dates_r['date'] = pd.to_datetime(dates_r['date'], errors='coerce')\n",
    "dates_r['date'].hist()\n",
    "print(sum(dates_r['date'] <= pd.to_datetime('2010-01-04')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_b = pd.read_csv(BLOOMBERG, usecols=[0, 1], index_col=0)\n",
    "dates_b['date'] = pd.to_datetime(dates_b['date'], errors='coerce')\n",
    "dates_b['date'].hist()\n",
    "print(sum(dates_b['date'] <= pd.to_datetime('2010-01-04')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before NYSE\n",
    "- All from Reuters and Bloomberg before first entry of NYSE dataset\n",
    "- 45363 article from Reuters\n",
    "- 1148 articles from Bloomber\n",
    "- Resulted in about 102.735 company occurrences\n",
    "- `./occurrences-before-nyse.csv` [3.9 MB]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reuters\n",
    "\n",
    "- Took 53h (2d 2h 58min 18s)\n",
    "- Analysed 106.519 articles (106.494 included content)\n",
    "- Resulted in 217.518 company occurrences\n",
    "- ... in 52.210 different articles\n",
    "- `./occurrences-reuters.csv`[8.5 MB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters = pd.read_csv(REUTERS, index_col=0)  # nrows=45363\n",
    "print(len(reuters))\n",
    "reuters = reuters[reuters['content'].notna()]\n",
    "print(len(reuters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(\n",
    "    index=range(5 * len(reuters)),\n",
    "    columns=['article_id', 'stock_symbol', 'match_text', 'start_idx', 'end_idx'])\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pbar = tqdm(reuters.iterrows(), total=len(reuters))\n",
    "for i, article in pbar:\n",
    "    # One full article takes about 7 seconds\n",
    "    found_entities = nlp_utils.find_nyse_corporations(article[2], quiet=True)\n",
    "    for ent, symbol in found_entities:\n",
    "        results.iloc[counter] = (f'r{i}', symbol, ent.text, ent.start_char, ent.end_char)\n",
    "        counter += 1\n",
    "    if (counter % 500) + len(found_entities) != (counter + len(found_entities)) % 500:\n",
    "        results.dropna().to_csv('occurrences-reuters.csv')\n",
    "        pbar.set_description(f\"Stored {counter} entries\")\n",
    "results.dropna().to_csv('occurrences-reuters.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bloomberg\n",
    "- Took ?\n",
    "- Analysed 448.395 articles (447.769 included content)\n",
    "- Resulted in ? company occurrences\n",
    "- ... in ? different articles\n",
    "- `./occurrences-bloomberg.csv` [? MB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448395\n",
      "time: 17.7 s\n"
     ]
    }
   ],
   "source": [
    "bloombergs = pd.read_csv(BLOOMBERG, index_col=0)  # nrows=1148\n",
    "print(len(bloombergs))\n",
    "# bloombergs = bloombergs[bloombergs['content'].notna()]\n",
    "# print(len(bloombergs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.25 s\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(\n",
    "    index=range(10 * len(bloombergs)),\n",
    "    columns=['article_id', 'stock_symbol', 'match_text', 'start_idx', 'end_idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 34.2 ms\n"
     ]
    }
   ],
   "source": [
    "# _results = pd.read_csv('occurrences-b1.csv', index_col=0)\n",
    "# print(results.shape, _results.shape)\n",
    "# results.iloc[:len(_results)] = _results\n",
    "# TODO: set start to last article id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 28.2 ms\n"
     ]
    }
   ],
   "source": [
    "# x = results2.iloc[:counter]\n",
    "# x.index = x.index + 34996\n",
    "# results.iloc[34996:34996+counter] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "231abbc562e44fdaa943b05544aabdf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=400308), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 49003 entries\n",
      "Stored 50000 entries\n",
      "Stored 51006 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 53002 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 55001 entries\n",
      "Stored 56011 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 58000 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 60001 entries\n",
      "Stored 61000 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 63001 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 65012 entries\n",
      "Stored 66000 entries\n",
      "Stored 67001 entries\n"
     ]
    }
   ],
   "source": [
    "# start = 0\n",
    "# start = 48087\n",
    "counter = start\n",
    "pbar = tqdm(bloombergs.iloc[start:].iterrows(), total=len(bloombergs)-start)\n",
    "for i, article in pbar:\n",
    "    if article.content is np.nan:\n",
    "        continue\n",
    "    # One full article takes about 7 seconds\n",
    "    found_entities = nlp_utils.find_nyse_corporations(article.content, quiet=True)\n",
    "    \n",
    "    for ent, symbol in found_entities:\n",
    "        results.iloc[counter] = (f'b{i}', symbol, ent.text, ent.start_char, ent.end_char)\n",
    "        counter += 1\n",
    "    if (counter % 1000) - len(found_entities) != (counter - len(found_entities)) % 1000:\n",
    "        results.dropna().to_csv('occurrences-b1.csv')\n",
    "        pbar.set_description(f\"Stored {counter} entries\")\n",
    "        print(f\"Stored {counter} entries\")\n",
    "results.dropna().to_csv('occurrences-b1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove headlines from articles and the found entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters.loc['head_end'] = 0\n",
    "r = results.dropna()\n",
    "print('Before:', r.shape)\n",
    "# remove_meta = re.compile(r'(--.*\\n)+[\\n\\s]*')\n",
    "remove_meta = re.compile(r'-- (.*)\\n(?:--.*\\n)+[\\n\\s]*')\n",
    "for i, article in tqdm(reuters.iterrows(), total=len(reuters)):\n",
    "    match = remove_meta.match(article.content)\n",
    "    article.title_start_idx = match.start(1)\n",
    "    article.title_end_idx = match.end(1)\n",
    "    article.head_end_idx = match.end()\n",
    "    article_id = f'r{i}'\n",
    "    r = r[(r.article_id != article_id) | (r.start_idx >= article.head_end_idx) |\n",
    "          (r.start_idx.between(article.title_start_idx, article.title_end_idx) &\n",
    "           r.end_idx.between(article.title_start_idx, article.title_end_idx))]\n",
    "print('After:', r.shape)\n",
    "\n",
    "# bloombergs.loc['head_end'] = 0\n",
    "# for i, article in tqdm(bloombergs.iterrows(), total=len(bloombergs)):\n",
    "#     article.head_end = remove_meta.match(article.content).end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply on Reuters Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# idxmax reuters: 61727  (FB 26, MSFT 1, NWSA 44, NWS 44, YHOO 1)\n",
    "reuters = pd.read_csv(REUTERS, skiprows=61727, nrows=1, index_col=0)\n",
    "print(reuters.loc[61727][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article1 = nlp(reuters.loc[61727][2])\n",
    "labels = [x.label_ for x in article1.ents]\n",
    "print(Counter(labels))\n",
    "items = [x.text for x in article1.ents if x.label_ == 'ORG']\n",
    "print(Counter(items))  # .most_common(3)\n",
    "sentences = [x for x in article1.sents]\n",
    "print(sentences[20])\n",
    "displacy.render(nlp(str(sentences[20])), jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counts = Counter([ent for ent, label in zip(items, labels) if label == 'ORG'])\n",
    "matches = [\n",
    "    [key, counts[key], securities_ds.get_most_similar_company(key)] for key in counts]\n",
    "matches = [x for x in matches if x[2] is not None]\n",
    "matched_stocks = dict([(x[0], x[2]) for x in matches])\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "securities_ds.get_most_similar_company('AOL-Time Warner', quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "found_entities = nlp_utils.find_nyse_corporations(reuters.loc[61727][2], quiet=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply on Bloomberg Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# idxmax bloomberg: 316777  (AAPL 1, FB 79, JPM 1, MSFT 1, MS 7)\n",
    "bloomberg = pd.read_csv(BLOOMBERG, skiprows=316777, nrows=1, index_col=0)\n",
    "# print(bloomberg.loc[316777][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_entities = nlp_utils.find_nyse_corporations(bloomberg.loc[316777][2], quiet=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "notify_time": "5"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
