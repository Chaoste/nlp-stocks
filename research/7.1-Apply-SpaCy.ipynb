{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import sys\n",
    "sys.path.append(\"..\") # Adds higher directory to python modules path for importing from src dir\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autotime\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 20.1 ms\n"
     ]
    }
   ],
   "source": [
    "HOME = \"..\"\n",
    "DATA_DIR = \"data\"\n",
    "REUTERS = os.path.join(HOME, DATA_DIR, \"preprocessed\", \"news_reuters.csv\")\n",
    "BLOOMBERG = os.path.join(HOME, DATA_DIR, \"preprocessed\", \"news_bloomberg.csv\")\n",
    "NEWS = os.path.join(HOME, DATA_DIR, \"preprocessed\", \"news.csv\")\n",
    "# Columns: 'date', 'filename', 'content'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 558 ms\n"
     ]
    }
   ],
   "source": [
    "from src.datasets import NyseSecuritiesDataset\n",
    "from src.datasets import NyseStocksDataset\n",
    "import src.nlp_utils as nlp_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 42.2 ms\n"
     ]
    }
   ],
   "source": [
    "securities_ds = NyseSecuritiesDataset(file_path='../data/nyse/securities.csv')\n",
    "companies = securities_ds.get_all_company_names()  # List[Tuple[symbol, name]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reuters:\n",
    "- 106519 articles\n",
    "- From 2006-10-20 to 2013-11-20\n",
    "- 45363 before 2010-01-04\n",
    "\n",
    "Bloomberg:\n",
    "- 448395 articles\n",
    "- From 2006-10-20 to 2013-11-26\n",
    "- 1148 before 2010-01-04\n",
    "\n",
    "Nyse:\n",
    "- From 2010-01-04 to 2016-12-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_r = pd.read_csv(REUTERS, usecols=[0, 1], index_col=0)\n",
    "dates_r['date'] = pd.to_datetime(dates_r['date'], errors='coerce')\n",
    "dates_r['date'].hist()\n",
    "print(sum(dates_r['date'] <= pd.to_datetime('2010-01-04')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_b = pd.read_csv(BLOOMBERG, usecols=[0, 1], index_col=0)\n",
    "dates_b['date'] = pd.to_datetime(dates_b['date'], errors='coerce')\n",
    "dates_b['date'].hist()\n",
    "print(sum(dates_b['date'] <= pd.to_datetime('2010-01-04')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before NYSE\n",
    "- All from Reuters and Bloomberg before first entry of NYSE dataset\n",
    "- 45363 article from Reuters\n",
    "- 1148 articles from Bloomber\n",
    "- Resulted in about 102.735 company occurrences\n",
    "- `./occurrences-before-nyse.csv` [3.9 MB]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reuters\n",
    "\n",
    "- Took 53h (2d 2h 58min 18s)\n",
    "- Analysed 106.519 articles (106.494 included content)\n",
    "- Resulted in 217.518 company occurrences\n",
    "- ... in 52.210 different articles\n",
    "- `./occurrences-reuters.csv`[8.5 MB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106519\n",
      "106494\n",
      "time: 5.72 s\n"
     ]
    }
   ],
   "source": [
    "reuters = pd.read_csv(REUTERS, index_col=0)  # nrows=45363\n",
    "print(len(reuters))\n",
    "reuters = reuters[reuters['content'].notna()]\n",
    "print(len(reuters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(\n",
    "    index=range(5 * len(reuters)),\n",
    "    columns=['article_id', 'stock_symbol', 'match_text', 'start_idx', 'end_idx'])\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pbar = tqdm(reuters.iterrows(), total=len(reuters))\n",
    "for i, article in pbar:\n",
    "    # One full article takes about 7 seconds\n",
    "    found_entities = nlp_utils.find_nyse_corporations(article[2], quiet=True)\n",
    "    for ent, symbol in found_entities:\n",
    "        results.iloc[counter] = (f'r{i}', symbol, ent.text, ent.start_char, ent.end_char)\n",
    "        counter += 1\n",
    "    if (counter % 500) + len(found_entities) != (counter + len(found_entities)) % 500:\n",
    "        results.dropna().to_csv('occurrences-reuters.csv')\n",
    "        pbar.set_description(f\"Stored {counter} entries\")\n",
    "results.dropna().to_csv('occurrences-reuters.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bloomberg\n",
    "- Took ?\n",
    "- Analysed 448.395 articles (447.769 included content)\n",
    "- Resulted in ? company occurrences\n",
    "- ... in ? different articles\n",
    "- `./occurrences-bloomberg.csv` [? MB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448395\n",
      "time: 25.7 s\n"
     ]
    }
   ],
   "source": [
    "bloombergs = pd.read_csv(BLOOMBERG, index_col=0)  # nrows=1148\n",
    "print(len(bloombergs))\n",
    "# bloombergs = bloombergs[bloombergs['content'].notna()]\n",
    "# print(len(bloombergs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.25 s\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(\n",
    "    index=range(10 * len(bloombergs)),\n",
    "    columns=['article_id', 'stock_symbol', 'match_text', 'start_idx', 'end_idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 34.2 ms\n"
     ]
    }
   ],
   "source": [
    "# _results = pd.read_csv('occurrences-b1.csv', index_col=0)\n",
    "# print(results.shape, _results.shape)\n",
    "# results.iloc[:len(_results)] = _results\n",
    "# TODO: set start to last article id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 28.2 ms\n"
     ]
    }
   ],
   "source": [
    "# x = results2.iloc[:counter]\n",
    "# x.index = x.index + 34996\n",
    "# results.iloc[34996:34996+counter] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "231abbc562e44fdaa943b05544aabdf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=400308), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 49003 entries\n",
      "Stored 50000 entries\n",
      "Stored 51006 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 53002 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 55001 entries\n",
      "Stored 56011 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 58000 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 60001 entries\n",
      "Stored 61000 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 63001 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 65012 entries\n",
      "Stored 66000 entries\n",
      "Stored 67001 entries\n"
     ]
    }
   ],
   "source": [
    "# start = 0\n",
    "# start = 48087\n",
    "counter = start\n",
    "pbar = tqdm(bloombergs.iloc[start:].iterrows(), total=len(bloombergs)-start)\n",
    "for i, article in pbar:\n",
    "    if article.content is np.nan:\n",
    "        continue\n",
    "    # One full article takes about 7 seconds\n",
    "    found_entities = nlp_utils.find_nyse_corporations(article.content, quiet=True)\n",
    "    \n",
    "    for ent, symbol in found_entities:\n",
    "        results.iloc[counter] = (f'b{i}', symbol, ent.text, ent.start_char, ent.end_char)\n",
    "        counter += 1\n",
    "    if (counter % 1000) - len(found_entities) != (counter - len(found_entities)) % 1000:\n",
    "        results.dropna().to_csv('occurrences-b1.csv')\n",
    "        pbar.set_description(f\"Stored {counter} entries\")\n",
    "        print(f\"Stored {counter} entries\")\n",
    "results.dropna().to_csv('occurrences-b1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove headlines from articles and the found entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reuters Articles 106494\n",
      "Bloomberg Articles 447378\n",
      "time: 20.7 s\n"
     ]
    }
   ],
   "source": [
    "reuters = pd.read_csv(REUTERS, index_col=0)\n",
    "reuters = reuters[reuters.content.notna()]\n",
    "print('Reuters Articles', len(reuters))\n",
    "reuters['title_start_idx'] = 0\n",
    "reuters['title_end_idx'] = 0\n",
    "reuters['head_end_idx'] = 0\n",
    "\n",
    "# Don't remove entries with empty content because the bb occurrences are matching the original index values\n",
    "bloombergs = pd.read_csv(BLOOMBERG, index_col=0)\n",
    "bloombergs['title_start_idx'] = 0\n",
    "bloombergs['title_end_idx'] = 0\n",
    "bloombergs['head_end_idx'] = 0\n",
    "\n",
    "# The articles keep their IDs which is necessary for matching them with the occurrences\n",
    "# Both are not necessary for reuters\n",
    "bloombergs = bloombergs[bloombergs.content.notna()]  # 626 article with empty content, e.g. b821, b822, b835, ..., b382080\n",
    "# Regex wouldn't match because only the title's existing: e.g. b19841, b7498, b8401, b11860 (so far all have below 100 chars)\n",
    "# For reuters there are always more than 100 chars, but sometimes only the meta data (~1000 entries)\n",
    "bloombergs = bloombergs[bloombergs.content.str.len() > 100]  # 391 articles, regexes wouldn't match\n",
    "print('Bloomberg Articles', len(bloombergs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 416 ms\n"
     ]
    }
   ],
   "source": [
    "orig_occ_r = pd.read_csv('occurrences-reuters.csv', index_col=0)\n",
    "x = orig_occ_r\n",
    "# All 3406 links in reuters articles are those in the header for the article link (and one in r49915 to uscourts.gov)\n",
    "x = x[~x.match_text.str.contains('www.')]\n",
    "orig_occ_r = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(291934, 5)\n",
      "122 out of 111783 links are left\n",
      "time: 1.85 s\n"
     ]
    }
   ],
   "source": [
    "orig_occ_b = pd.read_csv('occurrences-b1.csv', index_col=0)\n",
    "# BB: Links starting before inde 400 are eiter \"http://ww.bloomberg...\" or \"Ministry of Economy, Trade and Industry  http://www.meti.go.jp\"\n",
    "x = orig_occ_b\n",
    "x = x[~x.match_text.str.contains(\"Ministry of Economy, Trade and Industry\")]\n",
    "x = x[~x.match_text.str.contains('http:') | (x.start_idx > 300)]\n",
    "print(x.shape)\n",
    "print(f'{x.match_text.str.contains(\"http:\").sum()} out of {orig_occ_b.match_text.str.contains(\"http:\").sum()} links are left')\n",
    "orig_occ_b = x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter matches from header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 23 ms\n"
     ]
    }
   ],
   "source": [
    "# remove_meta = re.compile(r'(--.*\\n)+[\\n\\s]*')\n",
    "remove_meta = re.compile(r'-- (.*)\\n(?:--.*\\n)+[\\n\\s]*')\n",
    "# remove_meta_2 = re.compile(r'[\\s\\S]*(--.*\\n)(?:(?:[^-]-[^-]|[^-])*\\n){4,}')\n",
    "remove_meta_2 = re.compile(r'[\\s\\S]{0,250}(--.*\\.html\\s*\\n)(?:[^-](?:-[^-])*|-(?:[^-]-)*){250}')\n",
    "\n",
    "def filter_meta_matches(r, article, article_id):\n",
    "    # If there's no occurence in this article, we're done\n",
    "    if not len(r[r.article_id == article_id]):\n",
    "        return r\n",
    "    match = remove_meta.match(article.content)\n",
    "    if match:\n",
    "        article.title_start_idx = match.start(1)\n",
    "        article.title_end_idx = match.end(1)\n",
    "        article.head_end_idx = match.end()\n",
    "        return r[(r.article_id != article_id) | (r.start_idx >= article.head_end_idx) |\n",
    "             (r.start_idx.between(article.title_start_idx, article.title_end_idx) &\n",
    "             r.end_idx.between(article.title_start_idx, article.title_end_idx))]\n",
    "    match = remove_meta_2.match(article.content)\n",
    "    if match:\n",
    "        article.title_start_idx = -1\n",
    "        article.title_end_idx = -1\n",
    "        article.head_end_idx = match.end(1)\n",
    "        return r[(r.article_id != article_id) | (r.start_idx >= article.head_end_idx)]\n",
    "    print(f\"No regex worked for article {article_id}\")\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before - Reuters: (214112, 5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eca16be8ae54228809faf82c4181846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=106494), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After - Reuters: (214109, 5)\n",
      "time: 58min 28s\n"
     ]
    }
   ],
   "source": [
    "occ_r = orig_occ_r\n",
    "print('Before - Reuters:', orig_occ_r.shape)\n",
    "\n",
    "for i, article in tqdm(reuters.iterrows(), total=len(reuters)):\n",
    "    occ_r = filter_meta_matches(occ_r, article, f'r{i}')\n",
    "\n",
    "print('After - Reuters:', occ_r.shape)\n",
    "# Reduced from 214112 to 214109\n",
    "occ_r.to_csv('occurrences-reuters-v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before - Bloomberg: (291934, 5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71aefb4f2d2f4bf38416102a7525b8a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=447378), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No regex worked for article b69482\n",
      "No regex worked for article b76189\n",
      "No regex worked for article b83057\n",
      "No regex worked for article b88059\n",
      "No regex worked for article b97550\n",
      "No regex worked for article b98027\n",
      "No regex worked for article b100132\n",
      "No regex worked for article b107901\n",
      "No regex worked for article b113886\n",
      "No regex worked for article b124492\n",
      "No regex worked for article b134323\n",
      "No regex worked for article b136283\n",
      "No regex worked for article b151695\n",
      "No regex worked for article b163888\n",
      "No regex worked for article b164659\n",
      "No regex worked for article b164663\n",
      "No regex worked for article b166778\n",
      "No regex worked for article b166962\n",
      "No regex worked for article b173433\n",
      "No regex worked for article b176106\n",
      "No regex worked for article b176183\n",
      "No regex worked for article b181211\n",
      "No regex worked for article b185876\n",
      "After - Bloomberg: (287426, 5)\n",
      "time: 4h 10min 56s\n"
     ]
    }
   ],
   "source": [
    "occ_b = orig_occ_b\n",
    "print('Before - Bloomberg:', orig_occ_b.shape)\n",
    "# Fails: b69482, b76189, b83057, b88059, b97550, b98027, b100132, b107901, b113886, b124492, b134323, b136283, b151695, b163888, b164659, b164663, b166778, b166962, b173433, b176106, b176183, b181211, b185876\n",
    "for i, article in tqdm(bloombergs.iterrows(), total=len(bloombergs)):\n",
    "    occ_b = filter_meta_matches(occ_b, article, f'b{i}')\n",
    "print('After - Bloomberg:', occ_b.shape)\n",
    "# Reduced from 403596 to , from 111778 links to links (which are not in the header)\n",
    "occ_b.to_csv('occurrences-bloomberg-v2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply on Reuters Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# idxmax reuters: 61727  (FB 26, MSFT 1, NWSA 44, NWS 44, YHOO 1)\n",
    "reuters = pd.read_csv(REUTERS, skiprows=61727, nrows=1, index_col=0)\n",
    "print(reuters.loc[61727][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article1 = nlp(reuters.loc[61727][2])\n",
    "labels = [x.label_ for x in article1.ents]\n",
    "print(Counter(labels))\n",
    "items = [x.text for x in article1.ents if x.label_ == 'ORG']\n",
    "print(Counter(items))  # .most_common(3)\n",
    "sentences = [x for x in article1.sents]\n",
    "print(sentences[20])\n",
    "displacy.render(nlp(str(sentences[20])), jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counts = Counter([ent for ent, label in zip(items, labels) if label == 'ORG'])\n",
    "matches = [\n",
    "    [key, counts[key], securities_ds.get_most_similar_company(key)] for key in counts]\n",
    "matches = [x for x in matches if x[2] is not None]\n",
    "matched_stocks = dict([(x[0], x[2]) for x in matches])\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "securities_ds.get_most_similar_company('AOL-Time Warner', quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "found_entities = nlp_utils.find_nyse_corporations(reuters.loc[61727][2], quiet=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply on Bloomberg Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# idxmax bloomberg: 316777  (AAPL 1, FB 79, JPM 1, MSFT 1, MS 7)\n",
    "bloomberg = pd.read_csv(BLOOMBERG, skiprows=316777, nrows=1, index_col=0)\n",
    "# print(bloomberg.loc[316777][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_entities = nlp_utils.find_nyse_corporations(bloomberg.loc[316777][2], quiet=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "notify_time": "5"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
