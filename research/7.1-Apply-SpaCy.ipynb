{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import sys\n",
    "sys.path.append(\"..\") # Adds higher directory to python modules path for importing from src dir\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autotime\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = \"..\"\n",
    "DATA_DIR = \"data\"\n",
    "REUTERS = os.path.join(HOME, DATA_DIR, \"preprocessed\", \"news_reuters.csv\")\n",
    "BLOOMBERG = os.path.join(HOME, DATA_DIR, \"preprocessed\", \"news_bloomberg.csv\")\n",
    "NEWS = os.path.join(HOME, DATA_DIR, \"preprocessed\", \"news.csv\")\n",
    "# Columns: 'date', 'filename', 'content'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets import NyseSecuritiesDataset\n",
    "from src.datasets import NyseStocksDataset\n",
    "import src.nlp_utils as nlp_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "securities_ds = NyseSecuritiesDataset(file_path='../data/nyse/securities.csv')\n",
    "companies = securities_ds.get_all_company_names()  # List[Tuple[symbol, name]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reuters:\n",
    "- 106519 articles\n",
    "- From 2006-10-20 to 2013-11-20\n",
    "- 45363 before 2010-01-04\n",
    "\n",
    "Bloomberg:\n",
    "- 448395 articles\n",
    "- From 2006-10-20 to 2013-11-26\n",
    "- 1148 before 2010-01-04\n",
    "\n",
    "Nyse:\n",
    "- From 2010-01-04 to 2016-12-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_r = pd.read_csv(REUTERS, usecols=[0, 1], index_col=0)\n",
    "dates_r['date'] = pd.to_datetime(dates_r['date'], errors='coerce')\n",
    "dates_r['date'].hist()\n",
    "print(sum(dates_r['date'] <= pd.to_datetime('2010-01-04')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_b = pd.read_csv(BLOOMBERG, usecols=[0, 1], index_col=0)\n",
    "dates_b['date'] = pd.to_datetime(dates_b['date'], errors='coerce')\n",
    "dates_b['date'].hist()\n",
    "print(sum(dates_b['date'] <= pd.to_datetime('2010-01-04')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before NYSE\n",
    "- All from Reuters and Bloomberg before first entry of NYSE dataset\n",
    "- 45363 article from Reuters\n",
    "- 1148 articles from Bloomber\n",
    "- Resulted in about 102.735 company occurrences\n",
    "- `./occurrences-before-nyse.csv` [3.9 MB]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reuters\n",
    "\n",
    "- Took 53h (2d 2h 58min 18s)\n",
    "- Analysed 106.519 articles (106.494 included content)\n",
    "- Resulted in 217.518 company occurrences\n",
    "- ... in 52.210 different articles\n",
    "- `./occurrences-reuters.csv`[8.5 MB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters = pd.read_csv(REUTERS, index_col=0)  # nrows=45363\n",
    "print(len(reuters))\n",
    "reuters = reuters[reuters['content'].notna()]\n",
    "print(len(reuters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(\n",
    "    index=range(5 * len(reuters)),\n",
    "    columns=['article_id', 'stock_symbol', 'match_text', 'start_idx', 'end_idx'])\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pbar = tqdm(reuters.iterrows(), total=len(reuters))\n",
    "for i, article in pbar:\n",
    "    # One full article takes about 7 seconds\n",
    "    found_entities = nlp_utils.find_nyse_corporations(article[2], quiet=True)\n",
    "    for ent, symbol in found_entities:\n",
    "        results.iloc[counter] = (f'r{i}', symbol, ent.text, ent.start_char, ent.end_char)\n",
    "        counter += 1\n",
    "    if (counter % 500) + len(found_entities) != (counter + len(found_entities)) % 500:\n",
    "        results.dropna().to_csv('occurrences-reuters.csv')\n",
    "        pbar.set_description(f\"Stored {counter} entries\")\n",
    "results.dropna().to_csv('occurrences-reuters.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bloomberg\n",
    "- Took ?\n",
    "- Analysed 448.395 articles (447.769 included content)\n",
    "- Resulted in ? company occurrences\n",
    "- ... in ? different articles\n",
    "- `./occurrences-bloomberg.csv` [? MB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bloombergs = pd.read_csv(BLOOMBERG, index_col=0)  # nrows=1148\n",
    "print(len(bloombergs))\n",
    "bloombergs = bloombergs[bloombergs['content'].notna()]\n",
    "print(len(bloombergs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(\n",
    "    index=range(10 * len(bloombergs)),\n",
    "    columns=['article_id', 'stock_symbol', 'match_text', 'start_idx', 'end_idx'])\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm(bloombergs.iterrows(), total=len(bloombergs))\n",
    "for i, article in pbar:\n",
    "    # One full article takes about 7 seconds\n",
    "    found_entities = nlp_utils.find_nyse_corporations(article[2], quiet=True)\n",
    "    for ent, symbol in foun\n",
    "    d_entities:\n",
    "        results.iloc[counter] = (f'b{i}', symbol, ent.text, ent.start_char, ent.end_char)\n",
    "        counter += 1\n",
    "    if (counter % 100) + len(found_entities) != (counter + len(found_entities)) % 100:\n",
    "        results.dropna().to_csv('occurrences-b1.csv')\n",
    "        pbar.set_description(f\"Stored {counter} entries\")\n",
    "results.dropna().to_csv('occurrences-b1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove headlines from articles and the found entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters.loc['head_end'] = 0\n",
    "bloombergs.loc['head_end'] = 0\n",
    "r = results.dropna()\n",
    "print('Before:', r.shape)\n",
    "print('www:', sum(r.match_text.str.contains('www.')), 'http:', sum(r.match_text.str.contains('http:')))\n",
    "# remove_meta = re.compile(r'(--.*\\n)+[\\n\\s]*')\n",
    "remove_meta = re.compile(r'-- (.*)\\n(?:--.*\\n)+[\\n\\s]*')\n",
    "\n",
    "def filter_meta_matches(article, article_id):\n",
    "    match = remove_meta.match(article.content)\n",
    "    article.title_start_idx = match.start(1)\n",
    "    article.title_end_idx = match.end(1)\n",
    "    article.head_end_idx = match.end()\n",
    "    r = r[(r.article_id != article_id) | (r.start_idx >= article.head_end_idx) |\n",
    "          (r.start_idx.between(article.title_start_idx, article.title_end_idx) &\n",
    "           r.end_idx.between(article.title_start_idx, article.title_end_idx))]\n",
    "\n",
    "for i, article in tqdm(reuters.iterrows(), total=len(reuters)):\n",
    "    filter_meta_matches(article, f'r{i}')\n",
    "\n",
    "print('After Reuters:', r.shape)\n",
    "print('www:', sum(r.match_text.str.contains('www.')), 'http:', sum(r.match_text.str.contains('http:')))\n",
    "    \n",
    "for i, article in tqdm(bloombergs.iterrows(), total=len(bloombergs)):\n",
    "    filter_meta_matches(article, f'b{i}')\n",
    "\n",
    "print('After Bloomberg:', r.shape)\n",
    "print('www:', sum(r.match_text.str.contains('www.')), 'http:', sum(r.match_text.str.contains('http:')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply on Reuters Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# idxmax reuters: 61727  (FB 26, MSFT 1, NWSA 44, NWS 44, YHOO 1)\n",
    "reuters = pd.read_csv(REUTERS, skiprows=61727, nrows=1, index_col=0)\n",
    "print(reuters.loc[61727][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article1 = nlp(reuters.loc[61727][2])\n",
    "labels = [x.label_ for x in article1.ents]\n",
    "print(Counter(labels))\n",
    "items = [x.text for x in article1.ents if x.label_ == 'ORG']\n",
    "print(Counter(items))  # .most_common(3)\n",
    "sentences = [x for x in article1.sents]\n",
    "print(sentences[20])\n",
    "displacy.render(nlp(str(sentences[20])), jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counts = Counter([ent for ent, label in zip(items, labels) if label == 'ORG'])\n",
    "matches = [\n",
    "    [key, counts[key], securities_ds.get_most_similar_company(key)] for key in counts]\n",
    "matches = [x for x in matches if x[2] is not None]\n",
    "matched_stocks = dict([(x[0], x[2]) for x in matches])\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "securities_ds.get_most_similar_company('AOL-Time Warner', quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "found_entities = nlp_utils.find_nyse_corporations(reuters.loc[61727][2], quiet=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply on Bloomberg Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# idxmax bloomberg: 316777  (AAPL 1, FB 79, JPM 1, MSFT 1, MS 7)\n",
    "bloomberg = pd.read_csv(BLOOMBERG, skiprows=316777, nrows=1, index_col=0)\n",
    "# print(bloomberg.loc[316777][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_entities = nlp_utils.find_nyse_corporations(bloomberg.loc[316777][2], quiet=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "notify_time": "5"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
