{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content:\n",
    "- Use SpaCy NLP to find companys\n",
    "- Match companys in news with stock companies (using regex)\n",
    "- Exclude matches in headers afterwards (using regex)\n",
    "- Apply NLP on one Reuters and one Bloomberg article as show cases\n",
    "- Fix matchings for 3M Company\n",
    "- Generate cooccurrences (value represents number of articles in which two companies occur together)\n",
    "\n",
    "#### TODO:\n",
    "- Other measures for cooccurrence?\n",
    "- Read through some articles to find a show case how it related to stock prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\thomas\\hpi\\ma\\venv\\lib\\importlib\\_bootstrap.py:205: RuntimeWarning: cymem.cymem.Pool size changed, may indicate binary incompatibility. Expected 48 from C header, got 64 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "c:\\users\\thomas\\hpi\\ma\\venv\\lib\\importlib\\_bootstrap.py:205: RuntimeWarning: cymem.cymem.Address size changed, may indicate binary incompatibility. Expected 24 from C header, got 40 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "c:\\users\\thomas\\hpi\\ma\\venv\\lib\\importlib\\_bootstrap.py:205: RuntimeWarning: cymem.cymem.Pool size changed, may indicate binary incompatibility. Expected 48 from C header, got 64 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "c:\\users\\thomas\\hpi\\ma\\venv\\lib\\importlib\\_bootstrap.py:205: RuntimeWarning: cymem.cymem.Address size changed, may indicate binary incompatibility. Expected 24 from C header, got 40 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import sys\n",
    "sys.path.append(\"..\") # Adds higher directory to python modules path for importing from src dir\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autotime\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 164 ms\n"
     ]
    }
   ],
   "source": [
    "HOME = \"..\"\n",
    "DATA_DIR = \"data\"\n",
    "REUTERS = os.path.join(HOME, DATA_DIR, \"preprocessed\", \"news_reuters.csv\")\n",
    "BLOOMBERG = os.path.join(HOME, DATA_DIR, \"preprocessed\", \"news_bloomberg.csv\")\n",
    "NEWS = os.path.join(HOME, DATA_DIR, \"preprocessed\", \"news.csv\")\n",
    "# Columns: 'date', 'filename', 'content'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.01 s\n"
     ]
    }
   ],
   "source": [
    "from src.datasets import NyseSecuritiesDataset\n",
    "from src.datasets import NyseStocksDataset\n",
    "import src.nlp_utils as nlp_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a1fa3398a3498185dd56825ba8f9d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=501), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "time: 16.2 s\n"
     ]
    }
   ],
   "source": [
    "stocks_ds = NyseStocksDataset(file_path='../data/nyse/prices-split-adjusted.csv')\n",
    "securities_ds = NyseSecuritiesDataset(file_path='../data/nyse/securities.csv')\n",
    "companies = securities_ds.get_all_company_names()  # List[Tuple[symbol, name]]\n",
    "stocks_ds.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reuters:\n",
    "- 106519 articles\n",
    "- From 2006-10-20 to 2013-11-20\n",
    "- 45363 before 2010-01-04\n",
    "\n",
    "Bloomberg:\n",
    "- 448395 articles\n",
    "- From 2006-10-20 to 2013-11-26\n",
    "- 1148 before 2010-01-04\n",
    "\n",
    "Nyse:\n",
    "- From 2010-01-04 to 2016-12-30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before NYSE\n",
    "- All from Reuters and Bloomberg before first entry of NYSE dataset\n",
    "- 45363 article from Reuters\n",
    "- 1148 articles from Bloomberg\n",
    "- Resulted in about 102.735 company occurrences\n",
    "- `./occurrences-before-nyse.csv` [3.9 MB]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reuters\n",
    "\n",
    "- Took 53h (2d 2h 58min 18s)\n",
    "- Analysed 106.519 articles (106.494 included content)\n",
    "- Resulted in 217.518 company occurrences\n",
    "- ... in 52.210 different articles\n",
    "- `./occurrences-reuters.csv`[8.5 MB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106519\n",
      "106494\n",
      "time: 5.72 s\n"
     ]
    }
   ],
   "source": [
    "reuters = pd.read_csv(REUTERS, index_col=0)  # nrows=45363\n",
    "print(len(reuters))\n",
    "reuters = reuters[reuters['content'].notna()]\n",
    "print(len(reuters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(\n",
    "    index=range(5 * len(reuters)),\n",
    "    columns=['article_id', 'stock_symbol', 'match_text', 'start_idx', 'end_idx'])\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pbar = tqdm(reuters.iterrows(), total=len(reuters))\n",
    "for i, article in pbar:\n",
    "    # One full article takes about 7 seconds\n",
    "    found_entities = nlp_utils.find_nyse_corporations(article[2], quiet=True)\n",
    "    for ent, symbol in found_entities:\n",
    "        results.iloc[counter] = (f'r{i}', symbol, ent.text, ent.start_char, ent.end_char)\n",
    "        counter += 1\n",
    "    if (counter % 500) + len(found_entities) != (counter + len(found_entities)) % 500:\n",
    "        results.dropna().to_csv('occurrences-reuters.csv')\n",
    "        pbar.set_description(f\"Stored {counter} entries\")\n",
    "results.dropna().to_csv('occurrences-reuters.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bloomberg\n",
    "- Took ~4 days\n",
    "- Analysed 448.395 articles (447.769 included content)\n",
    "- Resulted in ? company occurrences\n",
    "- ... in ? different articles\n",
    "- `./occurrences-bloomberg.csv` [? MB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448395\n",
      "time: 25.7 s\n"
     ]
    }
   ],
   "source": [
    "bloombergs = pd.read_csv(BLOOMBERG, index_col=0)  # nrows=1148\n",
    "print(len(bloombergs))\n",
    "# The final used indexes would be confused if this line is executed\n",
    "# bloombergs = bloombergs[bloombergs['content'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.25 s\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(\n",
    "    index=range(10 * len(bloombergs)),\n",
    "    columns=['article_id', 'stock_symbol', 'match_text', 'start_idx', 'end_idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Required if it was stopped during execution (restarting at the last save point)\n",
    "# _results = pd.read_csv('occurrences-bloomberg.csv', index_col=0)\n",
    "# print(results.shape, _results.shape)\n",
    "# results.iloc[:len(_results)] = _results\n",
    "# start = 48087\n",
    "\n",
    "counter = start\n",
    "pbar = tqdm(bloombergs.iloc[start:].iterrows(), total=len(bloombergs)-start)\n",
    "for i, article in pbar:\n",
    "    if article.content is np.nan:\n",
    "        continue\n",
    "    # One full article takes about 7 seconds\n",
    "    found_entities = nlp_utils.find_nyse_corporations(article.content, quiet=True)\n",
    "    \n",
    "    for ent, symbol in found_entities:\n",
    "        results.iloc[counter] = (f'b{i}', symbol, ent.text, ent.start_char, ent.end_char)\n",
    "        counter += 1\n",
    "    if (counter % 1000) - len(found_entities) != (counter - len(found_entities)) % 1000:\n",
    "        results.dropna().to_csv('occurrences-bloomberg.csv')\n",
    "        pbar.set_description(f\"Stored {counter} entries\")\n",
    "        print(f\"Stored {counter} entries\")\n",
    "results.dropna().to_csv('occurrences-bloomberg.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove headlines from articles and the found entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reuters Articles 106494\n",
      "Bloomberg Articles 447378\n",
      "time: 20.7 s\n"
     ]
    }
   ],
   "source": [
    "reuters = pd.read_csv(REUTERS, index_col=0)\n",
    "reuters = reuters[reuters.content.notna()]\n",
    "print('Reuters Articles', len(reuters))\n",
    "reuters['title_start_idx'] = 0\n",
    "reuters['title_end_idx'] = 0\n",
    "reuters['head_end_idx'] = 0\n",
    "\n",
    "# Don't remove entries with empty content because the bb occurrences are matching the original index values\n",
    "bloombergs = pd.read_csv(BLOOMBERG, index_col=0)\n",
    "bloombergs['title_start_idx'] = 0\n",
    "bloombergs['title_end_idx'] = 0\n",
    "bloombergs['head_end_idx'] = 0\n",
    "\n",
    "# The articles keep their IDs which is necessary for matching them with the occurrences\n",
    "# Both are not necessary for reuters\n",
    "bloombergs = bloombergs[bloombergs.content.notna()]  # 626 article with empty content, e.g. b821, b822, b835, ..., b382080\n",
    "# Regex wouldn't match because only the title's existing: e.g. b19841, b7498, b8401, b11860 (so far all have below 100 chars)\n",
    "# For reuters there are always more than 100 chars, but sometimes only the meta data (~1000 entries)\n",
    "bloombergs = bloombergs[bloombergs.content.str.len() > 100]  # 391 articles, regexes wouldn't match\n",
    "print('Bloomberg Articles', len(bloombergs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 416 ms\n"
     ]
    }
   ],
   "source": [
    "orig_occ_r = pd.read_csv('occurrences-reuters.csv', index_col=0)\n",
    "x = orig_occ_r\n",
    "# All 3406 links in reuters articles are those in the header for the article link (and one in r49915 to uscourts.gov)\n",
    "x = x[~x.match_text.str.contains('www.')]\n",
    "orig_occ_r = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(291934, 5)\n",
      "122 out of 111783 links are left\n",
      "time: 1.85 s\n"
     ]
    }
   ],
   "source": [
    "orig_occ_b = pd.read_csv('occurrences-bloomberg.csv', index_col=0)\n",
    "# BB: Links starting before inde 400 are eiter \"http://ww.bloomberg...\" or \"Ministry of Economy, Trade and Industry  http://www.meti.go.jp\"\n",
    "x = orig_occ_b\n",
    "x = x[~x.match_text.str.contains(\"Ministry of Economy, Trade and Industry\")]\n",
    "x = x[~x.match_text.str.contains('http:') | (x.start_idx > 300)]\n",
    "print(x.shape)\n",
    "print(f'{x.match_text.str.contains(\"http:\").sum()} out of {orig_occ_b.match_text.str.contains(\"http:\").sum()} links are left')\n",
    "orig_occ_b = x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter matches from header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "occ_r = orig_occ_r\n",
    "print('Before - Reuters:', orig_occ_r.shape)\n",
    "\n",
    "for i, article in tqdm(reuters.iterrows(), total=len(reuters)):\n",
    "    occ_r = filter_meta_matches(occ_r, article, f'r{i}')\n",
    "\n",
    "print('After - Reuters:', occ_r.shape)\n",
    "# Reduced from 214112 to 214109\n",
    "occ_r.to_csv('occurrences-reuters-v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "occ_b = orig_occ_b\n",
    "print('Before - Bloomberg:', orig_occ_b.shape)\n",
    "# Fails: b69482, b76189, b83057, b88059, b97550, b98027, b100132, b107901, b113886, b124492, b134323, b136283, b151695, b163888, b164659, b164663, b166778, b166962, b173433, b176106, b176183, b181211, b185876\n",
    "for i, article in tqdm(bloombergs.iterrows(), total=len(bloombergs)):\n",
    "    occ_b = filter_meta_matches(occ_b, article, f'b{i}')\n",
    "print('After - Bloomberg:', occ_b.shape)\n",
    "# Reduced from 403596 to , from 111778 links to links (which are not in the header)\n",
    "occ_b.to_csv('occurrences-bloomberg-v2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply on Reuters Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 943 ms\n"
     ]
    }
   ],
   "source": [
    "# idxmax reuters: 61727  (FB 26, MSFT 1, NWSA 44, NWS 44, YHOO 1)\n",
    "reuters = pd.read_csv(REUTERS, skiprows=61727, nrows=1, index_col=0)\n",
    "# print(reuters.loc[61727][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article1 = nlp(reuters.loc[61727][2])\n",
    "labels = [x.label_ for x in article1.ents]\n",
    "print(Counter(labels))\n",
    "items = [x.text for x in article1.ents if x.label_ == 'ORG']\n",
    "print(Counter(items))  # .most_common(3)\n",
    "sentences = [x for x in article1.sents]\n",
    "print(sentences[20])\n",
    "displacy.render(nlp(str(sentences[20])), jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counts = Counter([ent for ent, label in zip(items, labels) if label == 'ORG'])\n",
    "matches = [[key, counts[key], securities_ds.get_most_similar_company(key)] for key in counts]\n",
    "matches = [x for x in matches if x[2] is not None]\n",
    "matched_stocks = dict([(x[0], x[2]) for x in matches])\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(securities_ds.get_most_similar_company('AOL-Time Warner', quiet=False))\n",
    "found_entities = nlp_utils.find_nyse_corporations(reuters.loc[61727][2], quiet=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply on Bloomberg Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# idxmax bloomberg: 316777  (AAPL 1, FB 79, JPM 1, MSFT 1, MS 7)\n",
    "bloomberg = pd.read_csv(BLOOMBERG, skiprows=316777, nrows=1, index_col=0)\n",
    "# print(bloomberg.loc[316777][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_entities = nlp_utils.find_nyse_corporations(bloomberg.loc[316777][2], quiet=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix matches for short company name (3M matches too many strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.41 s\n"
     ]
    }
   ],
   "source": [
    "occ_b = pd.read_csv('occurrences-bloomberg-v2.csv', index_col=0)\n",
    "# 32189 entries, this problem seems to only exist for 3M co. [MMM]\n",
    "cleaned_up = occ_b[(occ_b['stock_symbol'] != 'MMM') | occ_b.match_text.str.contains('3M')]\n",
    "# cleaned_up.to_csv('occurrences-bloomberg-v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.92 s\n"
     ]
    }
   ],
   "source": [
    "occ_r = pd.read_csv('occurrences-reuters-v2.csv', index_col=0)\n",
    "# 5751 entries, this problem seems to only exist for 3M co. [MMM]\n",
    "cleaned_up = occ_r[(occ_r['stock_symbol'] != 'MMM') | occ_r.match_text.str.contains('3M')]\n",
    "# cleaned_up.to_csv('occurrences-reuters-v2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Cooccurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 19.6 s\n"
     ]
    }
   ],
   "source": [
    "occs_b = pd.read_csv('occurrences-bloomberg-v2.csv', index_col=0)\n",
    "occs_r = pd.read_csv('occurrences-reuters-v2.csv', index_col=0)\n",
    "occs = pd.concat([occs_b, occs_r]).reset_index(drop=True)\n",
    "coocs = get_cooccurrences(occs)\n",
    "# coocs.to_csv('cooccurrences.csv')\n",
    "# readable_coocurrences(coocs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and analysing high important articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article(idx):\n",
    "    return pd.read_csv(BLOOMBERG, skiprows=idx, nrows=1, index_col=0).loc[idx][2]\n",
    "\n",
    "reuters = pd.read_csv(REUTERS, index_col=0)\n",
    "reuters = reuters[reuters['content'].notna()]\n",
    "# bloomberg = pd.read_csv(BLOOMBERG, index_col=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "notify_time": "5"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
