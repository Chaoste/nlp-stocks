{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content:\n",
    "- Train text classifier on custom labels (market sematic)\n",
    "\n",
    "#### TODO:\n",
    "- Grid Testing for Parameters\n",
    "- SpaCy: Find all types of entities in all news articles and store in CSV (\"nlp(doc, disable=['parser', 'ner'])\")\n",
    "- NaiveBayes instead of LinearSVC\n",
    "- Validate features by looking at most important words for each class\n",
    "- Have a look into Temporal Correlation, ApEN & Cramers V, Hatemining, Related Work Text Classification\n",
    "- Remove all entities before Vectorizer\n",
    "- Add pretrained WordEmbedding (e.g. BERT)\n",
    "- Filename_to_id for reuters and bloomberg\n",
    "\n",
    "#### Update:\n",
    "- Replace CountVectorizer mit TfidfVectorizer\n",
    "- Don't trim to 200 words but by frequency (bag of words may be very large)\n",
    "- Prepare Data & Write Janna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "from datetime import datetime, timedelta\n",
    "import itertools\n",
    "import sys\n",
    "import gc\n",
    "import linecache\n",
    "import tracemalloc\n",
    "sys.path.append(\"..\") # Adds higher directory to python modules path for importing from src dir\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from scipy.sparse import issparse\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, classification_report\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autotime\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3709aee8b6a5417181a6d91170d41e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=470), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "time: 47.8 s\n"
     ]
    }
   ],
   "source": [
    "from src.datasets import NyseSecuritiesDataset\n",
    "from src.datasets import NyseStocksDataset\n",
    "import src.nlp_utils as nlp_utils\n",
    "import src.text_classification_utils as tc_utils\n",
    "\n",
    "tracemalloc.start(10)\n",
    "\n",
    "HOME = \"..\"\n",
    "DATA_DIR = \"data\"\n",
    "NEWS = os.path.join(HOME, DATA_DIR, \"preprocessed\", \"news-v2.csv\")\n",
    "OCCS = os.path.join(HOME, DATA_DIR, \"preprocessed\", \"occurrences\", \"occurrences.csv\")\n",
    "COOCCS = os.path.join(HOME, DATA_DIR, \"preprocessed\", \"occurrences\", \"cooccurrences.csv\")\n",
    "\n",
    "stocks_ds = NyseStocksDataset(file_path='../data/nyse/prices-split-adjusted.csv'); stocks_ds.load()\n",
    "securities_ds = NyseSecuritiesDataset(file_path='../data/nyse/securities.csv'); securities_ds.load()\n",
    "companies = securities_ds.get_all_company_names()  # List[Tuple[symbol, name]]\n",
    "\n",
    "occs_per_article = tc_utils.get_occs_per_article(OCCS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define final test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0572e4e2bd34ed7b4103dcb9b40fc21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=501), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "snapshot1 = tracemalloc.take_snapshot()\n",
    "# Also contains prices from train for look back\n",
    "stocks_test_ds = NyseStocksDataset(file_path='../data/nyse/prices-split-adjusted.csv', only_test=True, load=True)\n",
    "all_news = pd.read_csv(NEWS, index_col=0)\n",
    "# news_test = tc_utils.load_news_clipped(stocks_test_ds, look_back=0, forecast=30, file_path=REUTERS)\n",
    "\n",
    "def final_test(pipe, look_back=0, forecast=30, epsilon_daily_label=0.01, epsilon_overall_label=0.05, min_occurrences=5):\n",
    "    # 60k articles\n",
    "    news_test = tc_utils.load_news_clipped(stocks_test_ds, look_back, forecast, news=all_news)\n",
    "    rel_article_tuples_test = tc_utils.get_relevant_articles(\n",
    "        news_test, occs_per_article, securities_ds, min_occ=min_occurrences)\n",
    "    rel_article_tuples_test = [x for x in rel_article_tuples_test\n",
    "                               if stocks_test_ds.is_company_available(x[0])]\n",
    "\n",
    "    X_test = np.array([nlp_utils.get_plain_content(x[1]) for x in rel_article_tuples_test])\n",
    "    y_test = tc_utils.get_discrete_labels(\n",
    "        rel_article_tuples_test, stocks_test_ds, look_back=look_back, forecast=forecast,\n",
    "        epsilon_daily_label=epsilon_daily_label, epsilon_overall_label=epsilon_overall_label)\n",
    "    print('Test distribution:', ''.join([f'\"{cls}\": {sum(y_test == cls)} samples; ' for cls in [1, -1, 0]]))\n",
    "\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    return acc, mcc, y_pred\n",
    "snapshot2 = tracemalloc.take_snapshot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_top(top_stats, limit=3):\n",
    "#     snapshot = snapshot.filter_traces((\n",
    "#         tracemalloc.Filter(False, \"<frozen importlib._bootstrap>\"),\n",
    "#         tracemalloc.Filter(False, \"<unknown>\"),\n",
    "#     ))\n",
    "    print(\"Top %s lines\" % limit)\n",
    "    for index, stat in enumerate(top_stats[:limit], 1):\n",
    "        frame = stat.traceback[0]\n",
    "        # replace \"/path/to/module/file.py\" with \"module/file.py\"\n",
    "        filename = os.sep.join(frame.filename.split(os.sep)[-2:])\n",
    "        print(\"#%s: %s:%s: %.1f KiB\"\n",
    "              % (index, filename, frame.lineno, stat.size / 1024))\n",
    "        line = linecache.getline(frame.filename, frame.lineno).strip()\n",
    "        if line:\n",
    "            print('    %s' % line)\n",
    "\n",
    "    other = top_stats[limit:]\n",
    "    if other:\n",
    "        size = sum(stat.size for stat in other)\n",
    "        print(\"%s other: %.1f KiB\" % (len(other), size / 1024))\n",
    "    total = sum(stat.size for stat in top_stats)\n",
    "    print(\"Total allocated size: %.1f KiB\" % (total / 1024))\n",
    "top_stats = snapshot2.compare_to(snapshot1, 'lineno')\n",
    "display_top(top_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2588a41b99fc4433bae9a2b64e22c78f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------- \n",
      " time_delta=-100\n",
      "---------------------------------------- \n",
      " look_back=100; forecast=0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "971a0be119d34a7fb5f2b8f360b24189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=379299), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EPSILON_DAILY_LABEL = 0.01\n",
    "EPSILON_OVERALL_LABEL = 0.05\n",
    "# min_occ->amount_articles:\n",
    "# 5->15048, 4->18499, 3->23256, 2->30675, 1->45816\n",
    "MIN_OCCURRENCES = 5  # for one company\n",
    "MAX_ARTICLES = 20000 # with min_occ=1 we get up to 25k\n",
    "\n",
    "algorithm = GaussianNB()\n",
    "metrics = []\n",
    "pipes = []\n",
    "\n",
    "for time_delta in tqdm([x for x in [-100, -50, -30, -10, -5, -1, 1, 5, 10, 30, 50, 100] if x != 0]):\n",
    "    print('-'*40, '\\n', f'time_delta={time_delta}')\n",
    "    look_back = abs(min(time_delta, 0))\n",
    "    forecast = abs(max(time_delta, 0))\n",
    "    pipe, val_acc, val_mcc, train_acc, train_mcc, data = tc_utils.run(\n",
    "        stocks_ds, securities_ds, occs_per_article, news=all_news, time_delta=time_delta,\n",
    "        epsilon_daily_label=EPSILON_DAILY_LABEL, epsilon_overall_label=EPSILON_OVERALL_LABEL,\n",
    "        min_occurrences=MIN_OCCURRENCES, max_articles=MAX_ARTICLES, algorithm=algorithm)\n",
    "    print('Data shape:', len(data[0]), len(data[1]), len(data[2]), len(data[3]))\n",
    "    data = None\n",
    "    gc.collect()\n",
    "    test_acc, test_mcc, _ = final_test(\n",
    "        pipe, look_back=look_back, forecast=forecast, epsilon_daily_label=EPSILON_DAILY_LABEL,\n",
    "        epsilon_overall_label=EPSILON_OVERALL_LABEL, min_occurrences=MIN_OCCURRENCES)\n",
    "\n",
    "    metrics.append((time_delta, test_acc, test_mcc, val_acc, val_mcc, train_acc, train_mcc))\n",
    "    pipes.append(pipes)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame(metrics, columns=['time', 'test_acc', 'test_mcc', 'val_acc', 'val_mcc', 'train_acc', 'train_mcc']).set_index('time')\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "metrics_df[['test_acc', 'val_acc', 'train_acc']].plot(ax=axes[0], title='Accuracy', grid=True)\n",
    "metrics_df[['test_mcc', 'val_mcc', 'train_acc']].plot(ax=axes[1], title='MCC', grid=True)\n",
    "fig.suptitle('All News - Text Classification Metrics')\n",
    "fig.tight_layout()\n",
    "fig.savefig('all-news-test-classification-metrics-gb-2.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON_DAILY_LABEL = 0.01\n",
    "EPSILON_OVERALL_LABEL = 0.05\n",
    "# min_occ->amount_articles:\n",
    "# 5->15048, 4->18499, 3->23256, 2->30675, 1->45816\n",
    "MIN_OCCURRENCES = 5  # for one company\n",
    "MAX_ARTICLES = 3000 # with min_occ=1 we get up to 25k\n",
    "\n",
    "algorithm2 = GaussianNB()\n",
    "metrics2 = []\n",
    "pipes2 = []\n",
    "\n",
    "for time_delta in tqdm([x for x in [-100, -50, -30, -10, -5, -1, 1, 5, 10, 30, 50, 100] if x != 0]):\n",
    "    snapshot1 = tracemalloc.take_snapshot()\n",
    "    print('-'*40, '\\n', f'time_delta={time_delta}')\n",
    "    look_back = abs(min(time_delta, 0))\n",
    "    forecast = abs(max(time_delta, 0))\n",
    "    pipe, val_acc, val_mcc, train_acc, train_mcc, data = tc_utils.run(\n",
    "        stocks_ds, securities_ds, occs_per_article, news=all_news, time_delta=time_delta,\n",
    "        epsilon_daily_label=EPSILON_DAILY_LABEL, epsilon_overall_label=EPSILON_OVERALL_LABEL,\n",
    "        min_occurrences=MIN_OCCURRENCES, max_articles=MAX_ARTICLES, algorithm=algorithm2)\n",
    "    print('Data shape:', len(data[0]), len(data[1]), len(data[2]), len(data[3]))\n",
    "    test_acc, test_mcc, _ = final_test(\n",
    "        pipe, look_back=look_back, forecast=forecast, epsilon_daily_label=0.01,\n",
    "        epsilon_overall_label=0.05, min_occurrences=MIN_OCCURRENCES)\n",
    "    \n",
    "    metrics2.append((time_delta, test_acc, test_mcc, val_acc, val_mcc, train_acc, train_mcc))\n",
    "    pipes2.append(pipes)\n",
    "    \n",
    "    snapshot2 = tracemalloc.take_snapshot()\n",
    "    top_stats = snapshot2.compare_to(snapshot1, 'lineno')\n",
    "    print(\"[ Top 10 ]\")\n",
    "    for stat in top_stats[:10]:\n",
    "        print(stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "- Plot with acc & val_acc for features from 50 to 5000D (https://nlp.stanford.edu/pubs/glove.pdf)\n",
    "- Show misleading improvement by split_after_shuffle=True (will fail von the test set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial: https://towardsdatascience.com/machine-learning-for-text-classification-using-spacy-in-python-b276b4051a49\n",
    "\n",
    "### General Setting\n",
    "- Use {LOOK_BACK} last days until open of the {FORECAST} day in the future (for articles on weekends go back to friday)\n",
    "- Articles from NYSE start 2010-03-22 to Reuters end 2012-12-31 [not touched final test set will be 2013-01-01 to 2013-11-20 with 3901-2803=1098 articles]\n",
    "- Only use title and real body (with some exceptions because of regex failure)\n",
    "- TODO: Don't remove numbers, links, special characters from vectorizer\n",
    "\n",
    "### Experiment 1\n",
    "- LOOK_BACK = 30\n",
    "- FORECAST = 0\n",
    "- EPSILON_DAILY_LABEL = 0.01\n",
    "- EPSILON_OVERALL_LABEL = 0.05\n",
    "- Label \"1\": 829 samples, Label \"-1\": 1017 samples, Label \"0\": 957 samples\n",
    "- Train: 2242 out of 2803 shuffled samples (Test: 561 samples)\n",
    "- LinearSVC warns: \"ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\"\n",
    "- $val\\_accuray=0.5$\n",
    "- $val\\_mcc=0.25$\n",
    "\n",
    "##### Experiment 2:\n",
    "- Tried calculating the mean of the relative diffs -> Values are very close to zero.\n",
    "- Therefore stick to the previous method. Calculate daily label and take the mean label.\n",
    "\n",
    "##### Experiment 3:\n",
    "- LOOK_BACK=7\n",
    "- Label \"1\": 991 labels\n",
    "- Label \"-1\": 1106 labels\n",
    "- Label \"0\": 706 labels\n",
    "- $val\\_accuray: 0.50$\n",
    "- $val\\_mcc: 0.23$\n",
    "\n",
    "##### Experiment 4:\n",
    "- LOOK_BACK=3\n",
    "- Label \"1\": 834 labels, Label \"-1\": 920 labels, Label \"0\": 1049 labels\n",
    "- $val\\_accuray: 0.41$\n",
    "- $val\\_mcc: 0.11$\n",
    "\n",
    "##### Experiment 5:\n",
    "- LOOK_BACK=1\n",
    "- Label \"1\": 620 labels,  Label \"-1\": 653 labels,  Label \"0\": 1530 labels\n",
    "- $val\\_accuray: 0.47$\n",
    "- $val\\_mcc: 0.12$\n",
    "\n",
    "##### Experiment 6:\n",
    "- LOOK_BACK=60\n",
    "- Label \"1\": 559 labels, Label \"-1\": 835 labels, Label \"0\": 1323 labels\n",
    "- $val\\_accuray: 0.56$\n",
    "- $val\\_mcc: 0.30$\n",
    "\n",
    "##### Experiment 7:\n",
    "- LOOK_BACK=0, FORECAST=30\n",
    "- Label \"1\": 853 labels, Label \"-1\": 1031 labels, Label \"0\": 985 labels\n",
    "- $val\\_accuracy: 0.56$\n",
    "- $val\\_mcc: 0.34$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "notify_time": "5"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
