{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content:\n",
    "- Train text classifier on custom labels (market sematic)\n",
    "\n",
    "#### TODO:\n",
    "- filename_to_id for reuters and bloomberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext autotime\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "time: 164 ms\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import sys\n",
    "sys.path.append(\"..\") # Adds higher directory to python modules path for importing from src dir\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef\n",
    "from sklearn import metrics\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autotime\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "917552115c86459d9773b8245453f03c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=470), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "time: 9.1 s\n"
     ]
    }
   ],
   "source": [
    "from src.datasets import NyseSecuritiesDataset\n",
    "from src.datasets import NyseStocksDataset\n",
    "from src.datasets import NyseFundamentalsDataset\n",
    "import src.nlp_utils as nlp_utils\n",
    "import src.text_classification_utils as tc_utils\n",
    "\n",
    "HOME = \"..\"\n",
    "DATA_DIR = \"data\"\n",
    "REUTERS = os.path.join(HOME, DATA_DIR, \"preprocessed\", \"news_reuters.csv\")\n",
    "BLOOMBERG = os.path.join(HOME, DATA_DIR, \"preprocessed\", \"news_bloomberg.csv\")\n",
    "NEWS = os.path.join(HOME, DATA_DIR, \"preprocessed\", \"news.csv\")\n",
    "\n",
    "stocks_ds = NyseStocksDataset(file_path='../data/nyse/prices-split-adjusted.csv'); stocks_ds.load()\n",
    "securities_ds = NyseSecuritiesDataset(file_path='../data/nyse/securities.csv'); securities_ds.load()\n",
    "companies = securities_ds.get_all_company_names()  # List[Tuple[symbol, name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of news articles: 58539\n",
      "Amount after first filter: 58515\n",
      "time: 7.35 s\n"
     ]
    }
   ],
   "source": [
    "# skiprows: 48000 -> 10-03-22, 47000 -> 10-02-22, 45400 -> 10-01-05\n",
    "NEWS_ARTICLE_START = '2010-03-22'\n",
    "news = tc_utils.load_news(REUTERS, start_date=NEWS_ARTICLE_START)\n",
    "\n",
    "occs_per_article = tc_utils.get_occs_per_article()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 2803 relevant article tuples\n",
      "time: 17.9 s\n"
     ]
    }
   ],
   "source": [
    "# Get all articles with enough occurrences\n",
    "MIN_OCCURRENCES = 5  # for one company\n",
    "rel_article_tuples = tc_utils.get_relevant_articles(news, occs_per_article, securities_ds, min_occ=MIN_OCCURRENCES)\n",
    "# Remove those which are not available in the training dataset of stock prices\n",
    "rel_article_tuples = [x for x in rel_article_tuples if stocks_ds.is_company_available(x[0])]\n",
    "print(f'Selected {len(rel_article_tuples)} relevant article tuples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 159 ms\n"
     ]
    }
   ],
   "source": [
    "LOOK_BACK = 30\n",
    "FORECAST = 0\n",
    "EPSILON_DAILY_LABEL = 0.01\n",
    "EPSILON_OVERALL_LABEL = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faeaac26dfe8436591c9b0396d327024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2803), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated labels for 2803 articles\n",
      "Distribution: \n",
      "- Label \"1\": 829\n",
      "- Label \"-1\": 1017\n",
      "- Label \"0\": 957\n",
      "time: 1min 50s\n"
     ]
    }
   ],
   "source": [
    "continuous_labels = np.array([tc_utils.get_label(*x, stocks_ds, look_back=LOOK_BACK,\n",
    "                                                 forecast=FORECAST, epsilon=EPSILON_DAILY_LABEL)\n",
    "                              for x in tqdm(rel_article_tuples)])\n",
    "print(f'Generated labels for {len(rel_article_tuples)} articles')\n",
    "discrete_labels = tc_utils.categorize_labels(continuous_labels, epsilon=EPSILON_OVERALL_LABEL)\n",
    "print('Distribution:', ''.join([f'\\n- Label \"{cls}\": {sum(discrete_labels == cls)} labels' for cls in [1, -1, 0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 480 ms\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = tc_utils.split_shuffled(rel_article_tuples, discrete_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From tutorial\n",
    "https://towardsdatascience.com/machine-learning-for-text-classification-using-spacy-in-python-b276b4051a49\n",
    "\n",
    "### General Setting\n",
    "- Use {LOOK_BACK} last days until open of the {FORECAST} day in the future (for articles on weekends go back to friday)\n",
    "- Articles from NYSE start 2010-03-22 to Reuters end 2012-12-31 [not touched final test set will be 2013-01-01 to 2013-11-20 with 3901-2803=1098 articles]\n",
    "- Only use title and real body (with some exceptions because of regex failure)\n",
    "- Don't remove numbers, links, special characters from vectorizer\n",
    "\n",
    "### Experiment 1\n",
    "- LOOK_BACK = 30\n",
    "- FORECAST = 0\n",
    "- EPSILON_DAILY_LABEL = 0.01\n",
    "- EPSILON_OVERALL_LABEL = 0.05\n",
    "- Label \"1\": 829 samples\n",
    "- Label \"-1\": 1017 samples\n",
    "- Label \"0\": 957 samples\n",
    "- Train: 2242 out of 2803 shuffled samples (Test: 561 samples)\n",
    "- LinearSVC warns: \"ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\"\n",
    "\n",
    "##### Resulting metrics:\n",
    "- $Accuray=0.5$\n",
    "- $MCC=0.25$\n",
    "- classification_report:\n",
    "\n",
    "                  precision    recall  f1-score   support\n",
    "\n",
    "            -1.0       0.56      0.53      0.54       209\n",
    "             0.0       0.49      0.46      0.48       198\n",
    "             1.0       0.45      0.51      0.48       154\n",
    "\n",
    "       micro avg       0.50      0.50      0.50       561\n",
    "       macro avg       0.50      0.50      0.50       561\n",
    "    weighted avg       0.51      0.50      0.50       561\n",
    "   \n",
    "### Experiment 2:\n",
    "- Tried calculating the mean of the relative diffs -> Values are very close to zero.\n",
    "- Therefore stick to the previous method. Calculate daily label and take the mean label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "- Different look_back, forecast, epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 166 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=tc_utils.tokenizeText, ngram_range=(1,1))\n",
    "clf = LinearSVC()\n",
    "pipe = Pipeline([('cleanText', tc_utils.CleanTextTransformer()), ('vectorizer', vectorizer), ('clf', clf)])\n",
    "\n",
    "# train\n",
    "print(\"Training...\")\n",
    "pipe.fit(X_train, y_train)\n",
    "# test\n",
    "print(\"Testing...\")\n",
    "y_pred = pipe.predict(X_test)\n",
    "# tc_utils.inspect_vectorizer(vectorizer, clf, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"- Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "print(f\"- MCC: {matthews_corrcoef(y_test, y_pred):.2f}\")\n",
    "print(metrics.classification_report(y_test, y_pred).replace('\\n', '\\n    '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect completeness of NYSE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fundamentals_ds = NyseFundamentalsDataset(file_path='../data/nyse/fundamentals.csv');\n",
    "fund_data = fundamentals_ds.data()\n",
    "\n",
    "fund_symbols = set(fund_data['Ticker Symbol'].unique())\n",
    "securities_symbols = set(securities_ds.get_all_company_names()['Ticker symbol'].values)\n",
    "prices = pd.read_csv('../data/nyse/prices-split-adjusted.csv')\n",
    "prices_symbols = set(prices.symbol.unique())\n",
    "\n",
    "fund_companies_without_sec = fund_symbols - securities_symbols\n",
    "fund_companies_without_prices = fund_symbols - prices_symbols\n",
    "sec_comp_without_fund = securities_symbols - fund_symbols\n",
    "sec_comp_without_prices = securities_symbols - prices_symbols\n",
    "prices_comp_without_fund = prices_symbols - fund_symbols\n",
    "prices_comp_without_sec = prices_symbols - securities_symbols\n",
    "\n",
    "print(f'Fund companies: {len(fund_symbols)}, Sec companies: {len(securities_symbols)}, Price companies: {len(prices_symbols)}\\n')\n",
    "print('fund_companies_without_sec:', sorted(fund_companies_without_sec), '\\n')\n",
    "print('fund_companies_without_prices:', sorted(fund_companies_without_prices), '\\n')\n",
    "print('sec_comp_without_fund:', sorted(sec_comp_without_fund), '\\n')\n",
    "print('sec_comp_without_prices:', sorted(sec_comp_without_prices), '\\n')\n",
    "print('prices_comp_without_fund:', sorted(prices_comp_without_fund), '\\n')\n",
    "print('prices_comp_without_sec:', sorted(prices_comp_without_sec), '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "notify_time": "5"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
