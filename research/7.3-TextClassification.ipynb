{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content:\n",
    "- Train text classifier on custom labels (market sematic)\n",
    "\n",
    "#### TODO:\n",
    "- filename_to_id for reuters and bloomberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext autotime\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "time: 250 ms\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import sys\n",
    "sys.path.append(\"..\") # Adds higher directory to python modules path for importing from src dir\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef\n",
    "from sklearn import metrics\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autotime\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9b3ac240439440e98c642652b6314a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=470), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "from src.datasets import NyseSecuritiesDataset\n",
    "from src.datasets import NyseStocksDataset\n",
    "import src.nlp_utils as nlp_utils\n",
    "import src.text_classification_utils as tc_utils\n",
    "\n",
    "HOME = \"..\"\n",
    "DATA_DIR = \"data\"\n",
    "REUTERS = os.path.join(HOME, DATA_DIR, \"preprocessed\", \"news_reuters.csv\")\n",
    "BLOOMBERG = os.path.join(HOME, DATA_DIR, \"preprocessed\", \"news_bloomberg.csv\")\n",
    "NEWS = os.path.join(HOME, DATA_DIR, \"preprocessed\", \"news.csv\")\n",
    "\n",
    "stocks_ds = NyseStocksDataset(file_path='../data/nyse/prices-split-adjusted.csv'); stocks_ds.load()\n",
    "securities_ds = NyseSecuritiesDataset(file_path='../data/nyse/securities.csv'); securities_ds.load()\n",
    "companies = securities_ds.get_all_company_names()  # List[Tuple[symbol, name]]\n",
    "\n",
    "occs_per_article = tc_utils.get_occs_per_article()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.22 s\n"
     ]
    }
   ],
   "source": [
    "LOOK_BACK = 0\n",
    "FORECAST = 30\n",
    "news = tc_utils.load_news_clipped(stocks_ds, LOOK_BACK, FORECAST, REUTERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define final test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 165 ms\n"
     ]
    }
   ],
   "source": [
    "stocks_test_ds = NyseStocksDataset(file_path='../data/nyse/prices-split-adjusted.csv', only_test=True, load=True)\n",
    "news_test = tc_utils.load_news_clipped(stocks_test_ds, look_back=0, forecast=30, file_path=REUTERS)\n",
    "\n",
    "def final_test(pipe, look_back=0, forecast=30, epsilon_daily_label=0.01, epsilon_overall_label=0.05, min_occurrences=5):\n",
    "    rel_article_tuples_test = tc_utils.get_relevant_articles(\n",
    "        news_test, occs_per_article, securities_ds, min_occ=min_occurrences)\n",
    "    rel_article_tuples_test = [x for x in rel_article_tuples_test\n",
    "                               if stocks_test_ds.is_company_available(x[0])]\n",
    "\n",
    "    X_test = np.array([nlp_utils.get_plain_content(x[1]) for x in rel_article_tuples_test])\n",
    "    y_test = tc_utils.get_discrete_labels(\n",
    "        rel_article_tuples_test, stocks_test_ds, look_back=look_back, forecast=forecast,\n",
    "        epsilon_daily_label=epsilon_daily_label, epsilon_overall_label=epsilon_overall_label)\n",
    "    print('Distribution:', ''.join([f'\"{cls}\": {sum(y_test == cls)} samples; ' for cls in [1, -1, 0]]))\n",
    "\n",
    "    y_pred = pipe.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    \n",
    "    return acc, mcc, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Select news with enough occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 2869 relevant article tuples\n",
      "time: 20.5 s\n"
     ]
    }
   ],
   "source": [
    "# Get all articles with enough occurrences for one company\n",
    "MIN_OCCURRENCES = 5\n",
    "rel_article_tuples = tc_utils.get_relevant_articles(news, occs_per_article, securities_ds, min_occ=MIN_OCCURRENCES)\n",
    "# Remove those which are not available in the training dataset of stock prices\n",
    "rel_article_tuples = [x for x in rel_article_tuples if stocks_ds.is_company_available(x[0])]\n",
    "print(f'Selected {len(rel_article_tuples)} relevant article tuples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e27c014245f34fdcb1dae39aefb0325e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2869), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated labels for 2869 articles\n",
      "Distribution: \n",
      "- Label \"1\": 853 labels\n",
      "- Label \"-1\": 1031 labels\n",
      "- Label \"0\": 985 labels\n",
      "time: 2min 8s\n"
     ]
    }
   ],
   "source": [
    "EPSILON_DAILY_LABEL = 0.01\n",
    "EPSILON_OVERALL_LABEL = 0.05\n",
    "LOOK_BACK = 0\n",
    "FORECAST = 30\n",
    "labels = tc_utils.get_discrete_labels(\n",
    "    rel_article_tuples, stocks_ds, look_back=LOOK_BACK, forecast=FORECAST,\n",
    "    epsilon_daily_label=EPSILON_DAILY_LABEL, epsilon_overall_label=EPSILON_OVERALL_LABEL)\n",
    "print(f'Generated labels for {len(labels)} articles')\n",
    "print('Distribution:', ''.join([f'\\n- Label \"{cls}\": {sum(discrete_labels == cls)} labels' for cls in [1, -1, 0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\thomas\\hpi\\ma\\venv\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = tc_utils.split_shuffled(rel_article_tuples, discrete_labels, split_after_shuffle=False)\n",
    "vectorizer = CountVectorizer(tokenizer=tc_utils.tokenizeText, ngram_range=(1,1), max_features=200)\n",
    "clf = LinearSVC(max_iter=5000)\n",
    "pipe = Pipeline([('cleanText', tc_utils.CleanTextTransformer()), ('vectorizer', vectorizer), ('clf', clf)])\n",
    "\n",
    "# train\n",
    "print(\"Training...\")\n",
    "pipe.fit(X_train, y_train)\n",
    "# test\n",
    "print(\"Testing...\")\n",
    "y_pred = pipe.predict(X_test)\n",
    "# tc_utils.inspect_vectorizer(vectorizer, clf, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Accuracy: 0.39\n",
      "- MCC: 0.08\n",
      "                    precision    recall  f1-score   support\n",
      "      \n",
      "              -1.0       0.38      0.52      0.44       193\n",
      "               0.0       0.45      0.37      0.41       216\n",
      "               1.0       0.32      0.27      0.29       165\n",
      "      \n",
      "         micro avg       0.39      0.39      0.39       574\n",
      "         macro avg       0.39      0.38      0.38       574\n",
      "      weighted avg       0.39      0.39      0.38       574\n",
      "      \n",
      "time: 198 ms\n"
     ]
    }
   ],
   "source": [
    "print(f\"- Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "print(f\"- MCC: {matthews_corrcoef(y_test, y_pred):.2f}\")\n",
    "print('     ', metrics.classification_report(y_test, y_pred).replace('\\n', '\\n      '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate on final test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecef405b36634f0398ad37250a10aac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1095), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution: \"1\": 484 samples; \"-1\": 295 samples; \"0\": 316 samples; \n",
      "time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "rel_article_tuples_test = tc_utils.get_relevant_articles(news_test, occs_per_article, securities_ds)\n",
    "rel_article_tuples_test = [x for x in rel_article_tuples_test if stocks_test_ds.is_company_available(x[0])]\n",
    "X_test = np.array([nlp_utils.get_plain_content(x[1]) for x in rel_article_tuples_test])\n",
    "y_test = tc_utils.get_discrete_labels(\n",
    "    rel_article_tuples_test, stocks_test_ds, look_back=0, forecast=30)\n",
    "print('Distribution:', ''.join([f'\"{cls}\": {sum(y_test == cls)} samples; ' for cls in [1, -1, 0]]))\n",
    "\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1095), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc, mcc, _ = final_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON_DAILY_LABEL = 0.01\n",
    "EPSILON_OVERALL_LABEL = 0.05\n",
    "MIN_OCCURRENCES = 5  # for one company\n",
    "\n",
    "metrics = []\n",
    "pipes = []\n",
    "\n",
    "for time_delta in tqdm([x for x in range(-90, 91, 50) if x != 0]):\n",
    "    print('-'*40, '\\n', f'time_delta={time_delta}')\n",
    "    look_back = abs(min(time_delta, 0))\n",
    "    forecast = abs(max(time_delta, 0))\n",
    "    continuous_labels = np.array([tc_utils.get_label(*x, stocks_ds, look_back=look_back, forecast=forecast)\n",
    "                                  for x in tqdm(rel_article_tuples)])\n",
    "    discrete_labels = categorize_labels(continuous_labels, epsilon=epsilon_overall_label)\n",
    "    pipe, acc, mcc = tc_utils.run(\n",
    "        stocks_ds, securities_ds, news, occs_per_article, time_delta=time_delta,\n",
    "        epsilon_daily_label=EPSILON_DAILY_LABEL, epsilon_overall_label=EPSILON_OVERALL_LABEL,\n",
    "        min_occurrences=MIN_OCCURRENCES)\n",
    "    test_acc, test_mcc, _ = final_test(pipe, look_back=look_back, forecast=forecast, epsilon_daily_label=0.01, epsilon_overall_label=0.05, min_occurrences=5\n",
    "    metrics.append((time_delta, acc, mcc, test_acc, test_mcc))\n",
    "    pipes.append(pipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pd.DataFrame(metrics, columns=['time', 'acc', 'mcc']).set_index('time').plot()\n",
    "ax.set_title('All News - Text Classification Metrics')\n",
    "# plt.gcf().savefig('all-news-test-classification-metrics.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_test_ds = NyseStocksDataset(file_path='../data/nyse/prices-split-adjusted.csv', only_test=True, load=True)\n",
    "news_test = pd.read_csv(NEWS, index_col=0)\n",
    "\n",
    "pipe, acc, mcc = tc_utils.run(\n",
    "    stocks_test_ds, securities_ds, news_test, occs_per_article, time_delta=30,\n",
    "    epsilon_daily_label=0.01, epsilon_overall_label=0.05, min_occurrences=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "- Plot with acc & val_acc for features from 50 to 5000D\n",
    "- Show misleading improvement by split_after_shuffle=True (will fail von the test set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial: https://towardsdatascience.com/machine-learning-for-text-classification-using-spacy-in-python-b276b4051a49\n",
    "\n",
    "### General Setting\n",
    "- Use {LOOK_BACK} last days until open of the {FORECAST} day in the future (for articles on weekends go back to friday)\n",
    "- Articles from NYSE start 2010-03-22 to Reuters end 2012-12-31 [not touched final test set will be 2013-01-01 to 2013-11-20 with 3901-2803=1098 articles]\n",
    "- Only use title and real body (with some exceptions because of regex failure)\n",
    "- Don't remove numbers, links, special characters from vectorizer\n",
    "\n",
    "### Experiment 1\n",
    "- LOOK_BACK = 30\n",
    "- FORECAST = 0\n",
    "- EPSILON_DAILY_LABEL = 0.01\n",
    "- EPSILON_OVERALL_LABEL = 0.05\n",
    "- Label \"1\": 829 samples\n",
    "- Label \"-1\": 1017 samples\n",
    "- Label \"0\": 957 samples\n",
    "- Train: 2242 out of 2803 shuffled samples (Test: 561 samples)\n",
    "- LinearSVC warns: \"ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\"\n",
    "\n",
    "###### Resulting metrics:\n",
    "- $Accuray=0.5$\n",
    "- $MCC=0.25$\n",
    "- classification_report:\n",
    "                    precision    recall  f1-score   support\n",
    "      \n",
    "              -1.0       0.54      0.52      0.53       209\n",
    "               0.0       0.49      0.47      0.48       198\n",
    "               1.0       0.46      0.50      0.48       154\n",
    "      \n",
    "         micro avg       0.50      0.50      0.50       561\n",
    "         macro avg       0.50      0.50      0.50       561\n",
    "      weighted avg       0.50      0.50      0.50       561\n",
    "   \n",
    "### Experiment 2:\n",
    "- Tried calculating the mean of the relative diffs -> Values are very close to zero.\n",
    "- Therefore stick to the previous method. Calculate daily label and take the mean label.\n",
    "\n",
    "### Experiment 3:\n",
    "- LOOK_BACK=7\n",
    "- Label \"1\": 991 labels\n",
    "- Label \"-1\": 1106 labels\n",
    "- Label \"0\": 706 labels\n",
    "\n",
    "###### Resulting metrics:\n",
    "- $Accuracy: 0.50$\n",
    "- $MCC: 0.23$\n",
    "\n",
    "### Experiment 4:\n",
    "- LOOK_BACK=3\n",
    "- Label \"1\": 834 labels\n",
    "- Label \"-1\": 920 labels\n",
    "- Label \"0\": 1049 labels\n",
    "\n",
    "###### Resulting metrics:\n",
    "- $Accuracy: 0.41$\n",
    "- $MCC: 0.11$\n",
    "\n",
    "### Experiment 5:\n",
    "- LOOK_BACK=1\n",
    "- Label \"1\": 620 labels\n",
    "- Label \"-1\": 653 labels\n",
    "- Label \"0\": 1530 labels\n",
    "\n",
    "###### Resulting metrics:\n",
    "- $Accuracy: 0.47$\n",
    "- $MCC: 0.12$\n",
    "\n",
    "### Experiment 6:\n",
    "- LOOK_BACK=60\n",
    "- Label \"1\": 559 labels\n",
    "- Label \"-1\": 835 labels\n",
    "- Label \"0\": 1323 labels\n",
    "\n",
    "###### Resulting metrics:\n",
    "- $Accuracy: 0.56$\n",
    "- $MCC: 0.30$\n",
    "\n",
    "### Experiment 7:\n",
    "- LOOK_BACK=0\n",
    "- FORECAST=30\n",
    "- Label \"1\": 853 labels\n",
    "- Label \"-1\": 1031 labels\n",
    "- Label \"0\": 985 labels\n",
    "- 2869 samples\n",
    "\n",
    "###### Resulting metrics:\n",
    "- $Accuracy: 0.56$\n",
    "- $MCC: 0.34$\n",
    "              precision    recall  f1-score   support\n",
    "    \n",
    "            -1.0       0.64      0.65      0.64       216\n",
    "             0.0       0.50      0.48      0.49       197\n",
    "             1.0       0.53      0.53      0.53       161\n",
    "    \n",
    "       micro avg       0.56      0.56      0.56       574\n",
    "       macro avg       0.56      0.56      0.56       574\n",
    "    weighted avg       0.56      0.56      0.56       574\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "notify_time": "5"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
