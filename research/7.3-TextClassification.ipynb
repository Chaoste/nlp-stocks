{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hi Janna,\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "All requried datasets can be downloaded from this link:\n",
    "https://drive.google.com/drive/folders/1KPRrLJ2HUz_QdPoz3TVGwz5ewH27FcxN?usp=sharing\n",
    "\n",
    "It contains:\n",
    "- Financial news article from Reuters & Bloomberg between\n",
    "\n",
    "\n",
    "Use {LOOK_BACK} last days until open of the {FORECAST} day in the future (for articles on weekends go back to friday)\n",
    "Articles from NYSE start 2010-03-22 to Reuters end 2012-12-31 [not touched final test set will be 2013-01-01 to 2013-11-20 with 3901-2803=1098 articles]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content:\n",
    "- Train text classifier on custom labels (market sematic)\n",
    "\n",
    "#### TODO:\n",
    "- Grid Testing for Parameters\n",
    "- SpaCy: Find all types of entities in all news articles and store in CSV (\"nlp(doc, disable=['parser', 'ner'])\")\n",
    "- NaiveBayes instead of LinearSVC\n",
    "- Validate features by looking at most important words for each class\n",
    "- Have a look into Temporal Correlation, ApEN & Cramers V, Hatemining, Related Work Text Classification\n",
    "- Remove all entities before Vectorizer\n",
    "- Add pretrained WordEmbedding (e.g. BERT)\n",
    "- Filename_to_id for reuters and bloomberg\n",
    "\n",
    "#### Update:\n",
    "- Replace CountVectorizer mit TfidfVectorizer\n",
    "- Don't trim to 200 words but by frequency (bag of words may be very large)\n",
    "- Prepare Data & Write Janna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import sys\n",
    "sys.path.append(\"..\") # Adds higher directory to python modules path for importing from src dir\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, classification_report\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autotime\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets import NyseSecuritiesDataset\n",
    "from src.datasets import NyseStocksDataset\n",
    "import src.nlp_utils as nlp_utils\n",
    "import src.text_classification_utils as tc_utils\n",
    "\n",
    "HOME = \"..\"\n",
    "DATA_DIR = \"data\"\n",
    "REUTERS = os.path.join(HOME, DATA_DIR, \"preprocessed\", \"news_reuters.csv\")\n",
    "BLOOMBERG = os.path.join(HOME, DATA_DIR, \"preprocessed\", \"news_bloomberg.csv\")\n",
    "NEWS = os.path.join(HOME, DATA_DIR, \"preprocessed\", \"news.csv\")\n",
    "\n",
    "stocks_ds = NyseStocksDataset(file_path='../data/nyse/prices-split-adjusted.csv'); stocks_ds.load()\n",
    "securities_ds = NyseSecuritiesDataset(file_path='../data/nyse/securities.csv'); securities_ds.load()\n",
    "companies = securities_ds.get_all_company_names()  # List[Tuple[symbol, name]]\n",
    "\n",
    "occs_per_article = tc_utils.get_occs_per_article()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOOK_BACK = 0\n",
    "FORECAST = 30\n",
    "news = tc_utils.load_news_clipped(stocks_ds, LOOK_BACK, FORECAST, REUTERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define final test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also contains prices from train for look back\n",
    "stocks_test_ds = NyseStocksDataset(file_path='../data/nyse/prices-split-adjusted.csv', only_test=True, load=True)\n",
    "all_news = pd.read_csv(NEWS, index_col=0)\n",
    "# news_test = tc_utils.load_news_clipped(stocks_test_ds, look_back=0, forecast=30, file_path=REUTERS)\n",
    "\n",
    "def final_test(pipe, look_back=0, forecast=30, epsilon_daily_label=0.01, epsilon_overall_label=0.05, min_occurrences=5):\n",
    "    news_test = tc_utils.load_news_clipped(stocks_test_ds, look_back, forecast, news=all_news)\n",
    "    rel_article_tuples_test = tc_utils.get_relevant_articles(\n",
    "        news_test, occs_per_article, securities_ds, min_occ=min_occurrences)\n",
    "    rel_article_tuples_test = [x for x in rel_article_tuples_test\n",
    "                               if stocks_test_ds.is_company_available(x[0])]\n",
    "\n",
    "    X_test = np.array([nlp_utils.get_plain_content(x[1]) for x in rel_article_tuples_test])\n",
    "    y_test = tc_utils.get_discrete_labels(\n",
    "        rel_article_tuples_test, stocks_test_ds, look_back=look_back, forecast=forecast,\n",
    "        epsilon_daily_label=epsilon_daily_label, epsilon_overall_label=epsilon_overall_label)\n",
    "    print('Test distribution:', ''.join([f'\"{cls}\": {sum(y_test == cls)} samples; ' for cls in [1, -1, 0]]))\n",
    "\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    return acc, mcc, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Select news with enough occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get all articles with enough occurrences for one company\n",
    "MIN_OCCURRENCES = 5\n",
    "rel_article_tuples = tc_utils.get_relevant_articles(news, occs_per_article, securities_ds, min_occ=MIN_OCCURRENCES)\n",
    "# Remove those which are not available in the training dataset of stock prices\n",
    "rel_article_tuples = [x for x in rel_article_tuples if stocks_ds.is_company_available(x[0])]\n",
    "print(f'Selected {len(rel_article_tuples)} relevant article tuples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPSILON_DAILY_LABEL = 0.01\n",
    "EPSILON_OVERALL_LABEL = 0.05\n",
    "LOOK_BACK = 0\n",
    "FORECAST = 30\n",
    "labels = tc_utils.get_discrete_labels(\n",
    "    rel_article_tuples, stocks_ds, look_back=LOOK_BACK, forecast=FORECAST,\n",
    "    epsilon_daily_label=EPSILON_DAILY_LABEL, epsilon_overall_label=EPSILON_OVERALL_LABEL)\n",
    "print(f'Generated labels for {len(labels)} articles')\n",
    "print('Distribution:', ''.join([f'\\n- Label \"{cls}\": {sum(discrete_labels == cls)} labels' for cls in [1, -1, 0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = tc_utils.split_shuffled(rel_article_tuples, discrete_labels, split_after_shuffle=False)\n",
    "# vectorizer = CountVectorizer(tokenizer=tc_utils.tokenizeText, ngram_range=(1,1), max_features=None)\n",
    "vectorizer = TfidfVectorizer(tokenizer=tc_utils.tokenizeText, ngram_range=(1,1), max_features=None)\n",
    "clf = LinearSVC(max_iter=10000)  # Naive Bayesian, LogisticCLassifiert\n",
    "pipe = Pipeline([('cleanText', tc_utils.CleanTextTransformer()), ('vectorizer', vectorizer), ('clf', clf)])\n",
    "\n",
    "# train\n",
    "print(\"Training...\")\n",
    "pipe.fit(X_train, y_train)\n",
    "# test\n",
    "print(\"Testing...\")\n",
    "y_pred = pipe.predict(X_test)\n",
    "# tc_utils.inspect_vectorizer(vectorizer, clf, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"- Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "print(f\"- MCC: {matthews_corrcoef(y_test, y_pred):.2f}\")\n",
    "print('     ', classification_report(y_test, y_pred).replace('\\n', '\\n      '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate on final test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_acc, test_mcc, _ = final_test(\n",
    "    pipe, look_back=LOOK_BACK, forecast=FORECAST, epsilon_daily_label=EPSILON_DAILY_LABEL,\n",
    "    epsilon_overall_label=EPSILON_OVERALL_LABEL, min_occurrences=MIN_OCCURRENCES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPSILON_DAILY_LABEL = 0.01\n",
    "EPSILON_OVERALL_LABEL = 0.05\n",
    "MIN_OCCURRENCES = 5  # for one company\n",
    "\n",
    "metrics = []\n",
    "pipes = []\n",
    "\n",
    "for time_delta in tqdm([x for x in range(-90, 91, 50) if x != 0]):\n",
    "    print('-'*40, '\\n', f'time_delta={time_delta}')\n",
    "    look_back = abs(min(time_delta, 0))\n",
    "    forecast = abs(max(time_delta, 0))\n",
    "    pipe, acc, mcc = tc_utils.run(\n",
    "        stocks_ds, securities_ds, news, occs_per_article, time_delta=time_delta,\n",
    "        epsilon_daily_label=EPSILON_DAILY_LABEL, epsilon_overall_label=EPSILON_OVERALL_LABEL,\n",
    "        min_occurrences=MIN_OCCURRENCES)\n",
    "    test_acc, test_mcc, _ = final_test(\n",
    "        pipe, look_back=look_back, forecast=forecast, epsilon_daily_label=0.01,\n",
    "        epsilon_overall_label=0.05, min_occurrences=5)\n",
    "    \n",
    "    metrics.append((time_delta, acc, mcc, test_acc, test_mcc))\n",
    "    pipes.append(pipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pd.DataFrame(metrics, columns=['time', 'val_acc', 'val_mcc', 'test_acc', 'test_mcc']).set_index('time').plot()\n",
    "ax.set_title('All News - Text Classification Metrics')\n",
    "# plt.gcf().savefig('all-news-test-classification-metrics.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_test_ds = NyseStocksDataset(file_path='../data/nyse/prices-split-adjusted.csv', only_test=True, load=True)\n",
    "news_test = pd.read_csv(NEWS, index_col=0)\n",
    "\n",
    "pipe, acc, mcc = tc_utils.run(\n",
    "    stocks_test_ds, securities_ds, news_test, occs_per_article, time_delta=30,\n",
    "    epsilon_daily_label=0.01, epsilon_overall_label=0.05, min_occurrences=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "- Plot with acc & val_acc for features from 50 to 5000D (https://nlp.stanford.edu/pubs/glove.pdf)\n",
    "- Show misleading improvement by split_after_shuffle=True (will fail von the test set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial: https://towardsdatascience.com/machine-learning-for-text-classification-using-spacy-in-python-b276b4051a49\n",
    "\n",
    "### General Setting\n",
    "- Use {LOOK_BACK} last days until open of the {FORECAST} day in the future (for articles on weekends go back to friday)\n",
    "- Articles from NYSE start 2010-03-22 to Reuters end 2012-12-31 [not touched final test set will be 2013-01-01 to 2013-11-20 with 3901-2803=1098 articles]\n",
    "- Only use title and real body (with some exceptions because of regex failure)\n",
    "- TODO: Don't remove numbers, links, special characters from vectorizer\n",
    "\n",
    "### Experiment 1\n",
    "- LOOK_BACK = 30\n",
    "- FORECAST = 0\n",
    "- EPSILON_DAILY_LABEL = 0.01\n",
    "- EPSILON_OVERALL_LABEL = 0.05\n",
    "- Label \"1\": 829 samples, Label \"-1\": 1017 samples, Label \"0\": 957 samples\n",
    "- Train: 2242 out of 2803 shuffled samples (Test: 561 samples)\n",
    "- LinearSVC warns: \"ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\"\n",
    "- $val\\_accuray=0.5$\n",
    "- $val\\_mcc=0.25$\n",
    "\n",
    "##### Experiment 2:\n",
    "- Tried calculating the mean of the relative diffs -> Values are very close to zero.\n",
    "- Therefore stick to the previous method. Calculate daily label and take the mean label.\n",
    "\n",
    "##### Experiment 3:\n",
    "- LOOK_BACK=7\n",
    "- Label \"1\": 991 labels\n",
    "- Label \"-1\": 1106 labels\n",
    "- Label \"0\": 706 labels\n",
    "- $val\\_accuray: 0.50$\n",
    "- $val\\_mcc: 0.23$\n",
    "\n",
    "##### Experiment 4:\n",
    "- LOOK_BACK=3\n",
    "- Label \"1\": 834 labels, Label \"-1\": 920 labels, Label \"0\": 1049 labels\n",
    "- $val\\_accuray: 0.41$\n",
    "- $val\\_mcc: 0.11$\n",
    "\n",
    "##### Experiment 5:\n",
    "- LOOK_BACK=1\n",
    "- Label \"1\": 620 labels,  Label \"-1\": 653 labels,  Label \"0\": 1530 labels\n",
    "- $val\\_accuray: 0.47$\n",
    "- $val\\_mcc: 0.12$\n",
    "\n",
    "##### Experiment 6:\n",
    "- LOOK_BACK=60\n",
    "- Label \"1\": 559 labels, Label \"-1\": 835 labels, Label \"0\": 1323 labels\n",
    "- $val\\_accuray: 0.56$\n",
    "- $val\\_mcc: 0.30$\n",
    "\n",
    "##### Experiment 7:\n",
    "- LOOK_BACK=0, FORECAST=30\n",
    "- Label \"1\": 853 labels, Label \"-1\": 1031 labels, Label \"0\": 985 labels\n",
    "- $val\\_accuracy: 0.56$\n",
    "- $val\\_mcc: 0.34$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "notify_time": "5"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
