{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content:\n",
    "- Use SpaCy NLP to find companys\n",
    "- Match companys in news with stock companies (using regex)\n",
    "- Exclude matches in headers afterwards (using regex)\n",
    "- Apply NLP on one Reuters and one Bloomberg article as show cases\n",
    "- Fix matchings for 3M Company\n",
    "- Generate cooccurrences (value represents number of articles in which two companies occur together)\n",
    "\n",
    "#### TODO:\n",
    "- Read through some articles to find a show case how it relates to stock prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import sys\n",
    "sys.path.append(\"..\") # Adds higher directory to python modules path for importing from src dir\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load(disable=['parser', 'tagger'])\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autotime\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = \"..\"\n",
    "DATA_DIR = \"data\"\n",
    "REUTERS = os.path.join(HOME, DATA_DIR, \"preprocessed\", \"news_reuters.csv\")\n",
    "BLOOMBERG = os.path.join(HOME, DATA_DIR, \"preprocessed\", \"news_bloomberg.csv\")\n",
    "NEWS = os.path.join(HOME, DATA_DIR, \"preprocessed\", \"news.csv\")\n",
    "# Columns: 'date', 'filename', 'content'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets import NyseSecuritiesDataset\n",
    "from src.datasets import NyseStocksDataset\n",
    "import src.nlp_utils as nlp_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_ds = NyseStocksDataset(file_path='../data/nyse/prices-split-adjusted.csv')\n",
    "securities_ds = NyseSecuritiesDataset(file_path='../data/nyse/securities.csv')\n",
    "companies = securities_ds.get_all_company_names()  # List[Tuple[symbol, name]]\n",
    "stocks_ds.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_entities(article, quiet=True):\n",
    "    ents = nlp(article.content).ents\n",
    "    title_start_idx, title_end_idx, head_end_idx = nlp_utils.get_meta_indizes(article)\n",
    "    for ent in ents:\n",
    "        s = ent.start_char\n",
    "        e = ent.end_char\n",
    "        if title_start_idx != -1:\n",
    "            if (title_start_idx <= s <= title_end_idx) or head_end_idx <= s:\n",
    "                yield ent\n",
    "        elif head_end_idx != -1:\n",
    "            if head_end_idx <= s:\n",
    "                yield ent\n",
    "        else:\n",
    "            yield ent\n",
    "    if not quiet:\n",
    "        displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_frame(arr, amount=None):\n",
    "    return pd.DataFrame(\n",
    "        arr,\n",
    "        index=range(amount or len(arr)),\n",
    "        columns=['article_id', 'match_text', 'start_idx', 'end_idx', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reuters:\n",
    "- 106519 articles\n",
    "- From 2006-10-20 to 2013-11-20\n",
    "- 45363 before 2010-01-04\n",
    "- Took 2h 26m\n",
    "- 6.788.173 entities\n",
    "- `./entities-reuters.csv`[273 MB]\n",
    "\n",
    "Bloomberg:\n",
    "- 448395 articles\n",
    "- From 2006-10-20 to 2013-11-26\n",
    "- 1148 before 2010-01-04\n",
    "-\n",
    "-\n",
    "-\n",
    "\n",
    "Nyse:\n",
    "- From 2010-01-04 to 2016-12-30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters = pd.read_csv(REUTERS, index_col=0)  # nrows=45363\n",
    "print(len(reuters))\n",
    "reuters = reuters[reuters['content'].notna()]\n",
    "print(len(reuters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pbar = tqdm(reuters.iterrows(), total=len(reuters))\n",
    "counter = 0\n",
    "results = []\n",
    "assert False, 'prevent overwriting existing entities file'\n",
    "for i, article in pbar:\n",
    "    for ent in find_entities(article):\n",
    "        results.append((f'r{i}', ent.text, ent.start_char, ent.end_char, ent.label_))\n",
    "        counter += 1\n",
    "    if (counter % 300000) + len(found_entities) != (counter + len(found_entities)) % 300000:\n",
    "        to_frame(results, counter).to_csv('entities-reuters.csv')\n",
    "        pbar.set_description(f\"Stored {counter} entities\")\n",
    "results = to_frame(results, counter).to_csv('entities-reuters.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bloomberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bloomberg = pd.read_csv(BLOOMBERG, index_col=0)  # nrows=1148\n",
    "print(len(bloomberg))\n",
    "# The final used indexes would be confused if this line is executed\n",
    "# bloomberg = bloomberg[bloomberg['content'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm(bloomberg.iterrows(), total=len(bloomberg))\n",
    "counter = 0\n",
    "results = []\n",
    "assert False, 'prevent overwriting existing entities file'\n",
    "for i, article in pbar:\n",
    "    if not isinstance(article.content, str):\n",
    "        continue\n",
    "    for ent in find_entities(article):\n",
    "        results.append((f'b{i}', ent.text, ent.start_char, ent.end_char, ent.label_))\n",
    "        counter += 1\n",
    "    if (counter % 300000) + len(found_entities) != (counter + len(found_entities)) % 300000:\n",
    "        to_frame(results, counter).to_csv('entities-bloomberg-1.csv')\n",
    "        pbar.set_description(f\"Stored {counter} entities\")\n",
    "results = to_frame(results, counter).to_csv('entities-bloomberg-1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 148877\n",
    "counter = 0\n",
    "pbar = tqdm(bloomberg.iloc[start:].iterrows(), total=len(bloomberg)-start)\n",
    "results = []\n",
    "assert False, 'prevent overwriting existing entities file'\n",
    "for i, article in pbar:\n",
    "    if not isinstance(article.content, str):\n",
    "        continue\n",
    "    prev_counter = counter\n",
    "    for ent in find_entities(article):\n",
    "        results.append((f'b{i}', ent.text, ent.start_char, ent.end_char, ent.label_))\n",
    "        counter += 1\n",
    "    if  (prev_counter % 500000) > (counter % 500000):\n",
    "        to_frame(results, counter).to_csv('entities-bloomberg-2.csv')\n",
    "        pbar.set_description(f\"Stored {counter} entities\")\n",
    "results = to_frame(results, counter).to_csv('entities-bloomberg-2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 335618\n",
    "counter = 0\n",
    "pbar = tqdm(bloomberg.iloc[start:].iterrows(), total=len(bloomberg)-start)\n",
    "results = []\n",
    "assert False, 'prevent overwriting existing entities file'\n",
    "for i, article in pbar:\n",
    "    if not isinstance(article.content, str):\n",
    "        continue\n",
    "    prev_counter = counter\n",
    "    for ent in find_entities(article):\n",
    "        results.append((f'b{i}', ent.text, ent.start_char, ent.end_char, ent.label_))\n",
    "        counter += 1\n",
    "    if  (prev_counter % 500000) > (counter % 500000):\n",
    "        to_frame(results, counter).to_csv('entities-bloomberg-3.csv')\n",
    "        pbar.set_description(f\"Stored {counter} entities\")\n",
    "results = to_frame(results, counter)\n",
    "results.to_csv('entities-bloomberg-3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Bloomberg Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After duplicating entities-bloomberg-1.csv\n",
    "pd.read_csv('entities-bloomberg-2.csv', index_col=0).to_csv('entities-bloomberg.csv', mode='a')\n",
    "pd.read_csv('entities-bloomberg-3.csv', index_col=0).to_csv('entities-bloomberg.csv', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.read_csv('entities-bloomberg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original: 33.914.235 entries -> After removing duplicates: 33.913.975 entries\n",
    "# Duplicate articles: b148877, b335618\n",
    "merged.drop_duplicates(subset=['article_id', 'start_idx'], inplace=True)\n",
    "merged.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_csv('entities-bloomberg.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply new mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv('news-v2.csv', usecols=['old_idx'])\n",
    "new_to_old_idx = news.old_idx.to_dict()\n",
    "old_to_new_idx = {v: k for k, v in new_to_old_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_reuters = pd.read_csv('entities-reuters.csv', index_col=0)\n",
    "entities_reuters.article_id = entities_reuters.article_id.apply(old_to_new_idx.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "entities_reuters.to_csv('entities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_bloomberg = pd.read_csv('entities-bloomberg.csv', index_col=0)\n",
    "entities_bloomberg.article_id = entities_bloomberg.article_id.apply(old_to_new_idx.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_bloomberg.to_csv('entities.csv', mode='a')\n",
    "# This also added the headline again and led to wrong dtypes:\n",
    "# Find the repeated headline:   awk '/^,article_id/ {print FNR}' entities.csv\n",
    "# Validate by printing it:   sed '6870969q; d' entities.csv\n",
    "# Delete line:   sed -i '6870969d' entities.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "40.702.148 Entities found in 554.068 articles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "notify_time": "5"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
