{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content:\n",
    "- Use SpaCy NLP to find companys\n",
    "- Match companys in news with stock companies (using regex)\n",
    "- Exclude matches in headers afterwards (using regex)\n",
    "- Apply NLP on one Reuters and one Bloomberg article as show cases\n",
    "- Fix matchings for 3M Company\n",
    "- Generate cooccurrences (value represents number of articles in which two companies occur together)\n",
    "\n",
    "#### TODO:\n",
    "- Read through some articles to find a show case how it relates to stock prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\thomas\\hpi\\ma\\venv\\lib\\importlib\\_bootstrap.py:205: RuntimeWarning: cymem.cymem.Pool size changed, may indicate binary incompatibility. Expected 48 from C header, got 64 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "c:\\users\\thomas\\hpi\\ma\\venv\\lib\\importlib\\_bootstrap.py:205: RuntimeWarning: cymem.cymem.Address size changed, may indicate binary incompatibility. Expected 24 from C header, got 40 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "c:\\users\\thomas\\hpi\\ma\\venv\\lib\\importlib\\_bootstrap.py:205: RuntimeWarning: cymem.cymem.Pool size changed, may indicate binary incompatibility. Expected 48 from C header, got 64 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "c:\\users\\thomas\\hpi\\ma\\venv\\lib\\importlib\\_bootstrap.py:205: RuntimeWarning: cymem.cymem.Address size changed, may indicate binary incompatibility. Expected 24 from C header, got 40 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import sys\n",
    "sys.path.append(\"..\") # Adds higher directory to python modules path for importing from src dir\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import src.nlp_utils as nlp_utils\n",
    "import src.text_classification_utils as tc_utils\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load(disable=['parser', 'tagger'])\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autotime\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 336 ms\n"
     ]
    }
   ],
   "source": [
    "HOME = \"..\"\n",
    "DATA_DIR = \"data\"\n",
    "REUTERS = os.path.join(HOME, DATA_DIR, \"preprocessed\", \"news_reuters.csv\")\n",
    "BLOOMBERG = os.path.join(HOME, DATA_DIR, \"preprocessed\", \"news_bloomberg.csv\")\n",
    "ENTITIES_v1 = os.path.join(\"..\", \"data\", \"preprocessed\", \"entities.csv\")\n",
    "ENTITIES_v2 = os.path.join(\"..\", \"data\", \"preprocessed\", \"entities-v2.csv\")\n",
    "ENTITIES = ENTITIES_v2\n",
    "OCCS = os.path.join(HOME, DATA_DIR, \"preprocessed\", \"occurrences\", \"occurrences.csv\")\n",
    "NEWS_v2 = os.path.join(HOME, DATA_DIR, \"preprocessed\", \"news-v2.csv\")\n",
    "NEWS_v3 = os.path.join(HOME, DATA_DIR, \"preprocessed\", \"news-v3.csv\")\n",
    "NEWS = NEWS_v3\n",
    "# Columns: 'date', 'filename', 'content'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Article Id Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv(NEWS, index_col=0)\n",
    "\n",
    "# 8650 duplicated filenames, 786 duplicated contents\n",
    "# d = news.filename.duplicated()\n",
    "# print(news.loc[1716].iloc[1].content) # 1201 letters\n",
    "# print(news.loc[1641].iloc[1].content) # 541 letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_old_idx(article):\n",
    "    if article.reuters:\n",
    "        return f'r{article.name}'\n",
    "    return f'b{article.name}'\n",
    "\n",
    "# [get_old_idx(idx, article) for idx, article in news.iterrows()];\n",
    "news['old_idx'] = news.apply(get_old_idx, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news.reset_index(drop=True, inplace=True)\n",
    "news = news[['old_idx', 'date', 'filename', 'content']]\n",
    "\n",
    "new_to_old_idx = news.old_idx.to_dict()\n",
    "old_to_new_idx = {v: k for k, v in new_to_old_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news.to_csv('news-v2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check for Occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv(NEWS_v2, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_to_old_idx = news.old_idx.to_dict()\n",
    "old_to_new_idx = {v: k for k, v in new_to_old_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = pd.read_csv('../data/preprocessed/occurrences/occurrences-bloomberg-v2.csv', index_col=0)\n",
    "re = pd.read_csv('../data/preprocessed/occurrences/occurrences-reuters-v2.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tests = 100000\n",
    "# for idx in tqdm(np.random.choice(len(re), n_tests, replace=False)):\n",
    "#     occurrence = re.iloc[idx]\n",
    "for idx in tqdm(np.random.choice(len(bb), n_tests, replace=False)):\n",
    "    occurrence = bb.iloc[idx]\n",
    "    str_in_article = news.loc[old_to_new_idx[occurrence.article_id]].content[occurrence.start_idx:occurrence.end_idx]\n",
    "    assert str_in_article == occurrence.match_text, f\"Didn't match for occurrence {idx}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.article_id = re.article_id.apply(old_to_new_idx.get)\n",
    "bb.article_id = bb.article_id.apply(old_to_new_idx.get)\n",
    "occurrences = pd.concat([re, bb])\n",
    "\n",
    "n_tests = 100000\n",
    "for idx in tqdm(np.random.choice(len(occurrences), n_tests, replace=False)):\n",
    "    occurrence = occurrences.iloc[idx]\n",
    "    str_in_article = news.loc[occurrence.article_id].content[occurrence.start_idx:occurrence.end_idx]\n",
    "    assert str_in_article == occurrence.match_text, f\"Didn't match for occurrence {idx}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check for Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = pd.read_csv(ENTITIES, index_col=0, nrows=10000)\n",
    "news = pd.read_csv(NEWS, index_col=0, nrows=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_txt_entities = entities[entities.match_text.isna()]\n",
    "txts = []\n",
    "for _, row in nan_txt_entities.iterrows():\n",
    "    txts.append(news.loc[row.article_id].content[row.start_idx:row.end_idx])\n",
    "txts = np.array(txts)\n",
    "print(np.unique(txts))  # -> N/A or NA with das transformed in real nan values\n",
    "# In entities.csv the files is empty -> e.g. line 2335779: \"2291077,39064,,1172,1174,ORG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tests = 10000\n",
    "for idx in tqdm(np.random.choice(len(entities), n_tests, replace=False)):\n",
    "    entity = entities.iloc[idx]\n",
    "    str_in_article = news3.loc[entity.article_id].content[entity.start_idx:entity.end_idx]\n",
    "    assert (isinstance(entity.match_text, float) and np.isnan(entity.match_text)) or str_in_article == entity.match_text, f\"Didn't match entity at {idx}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News-v3\n",
    "Reuters and Bloomberg are now mixed because they are sorted by date. This does not change the indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 41.1 s\n"
     ]
    }
   ],
   "source": [
    "news = pd.read_csv(NEWS_v2, index_col=0)\n",
    "news.date = pd.to_datetime(news.date)\n",
    "news = news.sort_values(by=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "with open(NEWS_v3, mode='w', newline='\\n', encoding='utf-8') as f:\n",
    "    news.to_csv(f, line_terminator='\\n', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing HDF5\n",
    "Performance roughly as good as csv. --> http://matthewrocklin.com/blog/work/2015/03/16/Fast-Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\thomas\\hpi\\ma\\venv\\lib\\site-packages\\pandas\\core\\generic.py:1471: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block0_values] [items->['old_idx', 'filename', 'content']]\n",
      "\n",
      "  return pytables.to_hdf(path_or_buf, key, self, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "news.to_hdf(NEWS_v3[:-3]+'h5', 'df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 48 s\n"
     ]
    }
   ],
   "source": [
    "news = pd.read_hdf(NEWS_v3[:-3]+'h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: v3 contained \\r\\r for each \\r because of Windows line formatting rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge Entities and Occs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv(NEWS_v3, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 40.8 s\n"
     ]
    }
   ],
   "source": [
    "entities = pd.read_csv(ENTITIES_v1, index_col=0)\n",
    "occs = pd.read_csv(OCCS, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "entities = nlp_utils.merge_entities_and_occs(entities, occs, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "entities.sort_values(by=['article_id', 'start_idx'], ascending=[True, False], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7min 6s\n"
     ]
    }
   ],
   "source": [
    "entities.to_csv(ENTITIES_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- 2.3 mio ORG entities start with an empty space. This might led to not matching stock symbols. (Example \" Exxon Mobile\")\n",
    "- Entity Labels: ['ORG', 'GPE', 'MONEY', 'DATE', 'QUANTITY', 'CARDINAL', 'NORP', 'PERSON', 'PRODUCT', 'LAW', 'PERCENT']\n",
    "- Company Occurrences are marked with < SYM >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\thomas\\hpi\\ma\\venv\\lib\\site-packages\\numpy\\lib\\arraysetops.py:522: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "entities = pd.read_csv(ENTITIES_v2, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 366 ms\n"
     ]
    }
   ],
   "source": [
    "grouped = entities.groupby('article_id', sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 211 ms\n"
     ]
    }
   ],
   "source": [
    "def remove_substr(s, to_idx, from_idx):\n",
    "    return s[:to_idx]+s[from_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d7e7888d1644e1c97c4cbe38b8248a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=554065), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for article_id, group in tqdm(grouped, total=len(grouped.groups)):\n",
    "    # group.sort_values(by='start_idx', inplace=True, ascending=False)\n",
    "    s = news.loc[article_id].content\n",
    "    for i, row in group.iterrows():\n",
    "        # print(news.loc[0].content[row.start_idx-2:row.end_idx+2])\n",
    "        # Plus 1 for end_idx so the next sign (probably a space) will also be removed\n",
    "        s = remove_substr(s, row.start_idx, row.end_idx+1)\n",
    "        # print(s[row.start_idx-2:row.end_idx+2])\n",
    "    news.loc[article_id].content = s\n",
    "    # print(s)\n",
    "    # assert False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "notify_time": "5"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
