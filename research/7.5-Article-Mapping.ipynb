{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content:\n",
    "- Use SpaCy NLP to find companys\n",
    "- Match companys in news with stock companies (using regex)\n",
    "- Exclude matches in headers afterwards (using regex)\n",
    "- Apply NLP on one Reuters and one Bloomberg article as show cases\n",
    "- Fix matchings for 3M Company\n",
    "- Generate cooccurrences (value represents number of articles in which two companies occur together)\n",
    "\n",
    "#### TODO:\n",
    "- Read through some articles to find a show case how it relates to stock prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import sys\n",
    "sys.path.append(\"..\") # Adds higher directory to python modules path for importing from src dir\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load(disable=['parser', 'tagger'])\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autotime\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = \"..\"\n",
    "DATA_DIR = \"data\"\n",
    "REUTERS = os.path.join(HOME, DATA_DIR, \"preprocessed\", \"news_reuters.csv\")\n",
    "BLOOMBERG = os.path.join(HOME, DATA_DIR, \"preprocessed\", \"news_bloomberg.csv\")\n",
    "NEWS = os.path.join(HOME, DATA_DIR, \"preprocessed\", \"news.csv\")\n",
    "# Columns: 'date', 'filename', 'content'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv(NEWS, index_col=0)\n",
    "\n",
    "# 8650 duplicated filenames, 786 duplicated contents\n",
    "# d = news.filename.duplicated()\n",
    "# print(news.loc[1716].iloc[1].content) # 1201 letters\n",
    "# print(news.loc[1641].iloc[1].content) # 541 letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_old_idx(article):\n",
    "    if article.reuters:\n",
    "        return f'r{article.name}'\n",
    "    return f'b{article.name}'\n",
    "\n",
    "# [get_old_idx(idx, article) for idx, article in news.iterrows()];\n",
    "news['old_idx'] = news.apply(get_old_idx, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news.reset_index(drop=True, inplace=True)\n",
    "news = news[['old_idx', 'date', 'filename', 'content']]\n",
    "\n",
    "new_to_old_idx = news.old_idx.to_dict()\n",
    "old_to_new_idx = {v: k for k, v in new_to_old_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news.to_csv('news-v2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check for Occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 29.4 s\n"
     ]
    }
   ],
   "source": [
    "NEWS = os.path.join(HOME, DATA_DIR, \"preprocessed\", \"news-v2.csv\")\n",
    "news = pd.read_csv(NEWS, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 630 ms\n"
     ]
    }
   ],
   "source": [
    "new_to_old_idx = news.old_idx.to_dict()\n",
    "old_to_new_idx = {v: k for k, v in new_to_old_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 704 ms\n"
     ]
    }
   ],
   "source": [
    "bb = pd.read_csv('../data/preprocessed/occurrences/occurrences-bloomberg-v2.csv', index_col=0)\n",
    "re = pd.read_csv('../data/preprocessed/occurrences/occurrences-reuters-v2.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91292768fde7417ba95db26f0cceb742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=100000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "time: 38.5 s\n"
     ]
    }
   ],
   "source": [
    "n_tests = 100000\n",
    "# for idx in tqdm(np.random.choice(len(re), n_tests, replace=False)):\n",
    "#     occurrence = re.iloc[idx]\n",
    "for idx in tqdm(np.random.choice(len(bb), n_tests, replace=False)):\n",
    "    occurrence = bb.iloc[idx]\n",
    "    str_in_article = news.loc[old_to_new_idx[occurrence.article_id]].content[occurrence.start_idx:occurrence.end_idx]\n",
    "    assert str_in_article == occurrence.match_text, f\"Didn't match for occurrence {idx}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3fc46059b514c02a011d9523edea4de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=100000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "time: 42.3 s\n"
     ]
    }
   ],
   "source": [
    "re.article_id = re.article_id.apply(old_to_new_idx.get)\n",
    "bb.article_id = bb.article_id.apply(old_to_new_idx.get)\n",
    "occurrences = pd.concat([re, bb])\n",
    "\n",
    "n_tests = 100000\n",
    "for idx in tqdm(np.random.choice(len(occurrences), n_tests, replace=False)):\n",
    "    occurrence = occurrences.iloc[idx]\n",
    "    str_in_article = news.loc[occurrence.article_id].content[occurrence.start_idx:occurrence.end_idx]\n",
    "    assert str_in_article == occurrence.match_text, f\"Didn't match for occurrence {idx}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check for Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITIES = os.path.join(\"..\", \"data\", \"preprocessed\", \"entities.csv\")\n",
    "NEWS = os.path.join(\"..\", \"data\", \"preprocessed\", \"news-v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = pd.read_csv(ENTITIES, index_col=0)\n",
    "news = pd.read_csv(NEWS, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.34 s\n"
     ]
    }
   ],
   "source": [
    "nan_txt_entities = entities[entities.match_text.isna()]\n",
    "txts = []\n",
    "for _, row in nan_txt_entities.iterrows():\n",
    "    txts.append(news.loc[row.article_id].content[row.start_idx:row.end_idx])\n",
    "txts = np.array(txts)\n",
    "print(np.unique(txts))  # -> N/A or NA with das transformed in real nan values\n",
    "# In entities.csv the files is empty -> e.g. line 2335779: \"2291077,39064,,1172,1174,ORG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a89bdde78844213a7faf345f37e96a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=100000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2min 45s\n"
     ]
    }
   ],
   "source": [
    "n_tests = 100000\n",
    "for idx in tqdm(np.random.choice(len(entities), n_tests, replace=False)):\n",
    "    entity = entities.iloc[idx]\n",
    "    str_in_article = news.loc[entity.article_id].content[entity.start_idx:entity.end_idx]\n",
    "    assert (isinstance(entity.match_text, float) and np.isnan(entity.match_text)) or str_in_article == entity.match_text, f\"Didn't match entity at {idx}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News-v3\n",
    "Reuters and Bloomberg are now mixed because they are sorted by date. This does not change the indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv(NEWS, index_col=0)\n",
    "news.date = pd.to_datetime(all_news.date)\n",
    "news = all_news.sort_values(by=['date'])\n",
    "news.to_csv('../data/preprocessed/news-v3.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "notify_time": "5"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
